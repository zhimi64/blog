<!doctype html>
<html lang="" itemscope itemtype="http://schema.org/Person">
<head>
  

  <meta charset="utf-8">
  <!-- Site Meta Data -->
  <title>手寫Multi-Head Attention：從公式到代碼實現</title>
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="Hi，我是“執迷”，一位算法工程師。我關注計算機視覺、自然語言處理領域的最新問題，分享有趣的技術和經驗。歡迎訪問我的博客！">
  <meta name="author" content="執迷">

  <link rel="shortcut icon" href="favicon.ico">

  <!-- schema.org -->
  <meta itemprop="name" content="執迷的博客">
  <meta itemprop="image" content="">
  <meta itemprop="description" content="Hi，我是“執迷”，一位算法工程師。我關注計算機視覺、自然語言處理領域的最新問題，分享有趣的技術和經驗。歡迎訪問我的博客！">

  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,600,700' rel='stylesheet' type='text/css'>
  <!-- Style Meta Data -->
  <link rel="stylesheet" href="./theme/css/style.css" type="text/css" />
  <link rel="stylesheet" href="./theme/css/pygments.css" type="text/css" />

  <!-- Feed Meta Data -->
  
  

  <!-- Twitter Feed -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="">
  <meta name="twitter:image" content="">
  
<meta name="twitter:creator" content="">
<meta name="twitter:url" content="././multi_head_20240311.html">
<meta name="twitter:title" content="執迷的博客 ~ 手寫Multi-Head Attention：從公式到代碼實現">
<meta name="twitter:description" content="">

<!-- Facebook Meta Data -->
<meta property="og:title" content="執迷的博客 ~ 手寫Multi-Head Attention：從公式到代碼實現" />
<meta property="og:description" content="" />
<meta property="og:image" content="" />

</head>

<body>
  <!-- Sidebar -->
  <aside>
    <!--<center><a href="."><img id="avatar" src=""></a></center>-->
    <h1>執迷的博客</h1>
    
      <p>做一個有趣的人！</p>
    
    <br>

    

    <nav class="nav">
      <ul class="list-bare">
      
        
          <li><a class="nav__link" href="./index.html">Blog</a></li>
        
         
        
          <li><a class="nav__link" href="././about.html">About</a></li>
        
         
      </ul>
    </nav>

    <p class="social">
      
      
      
    </p>

    <!--
    
    -->
  </aside>

  <!-- Content -->
  <article>
    
<section id="content">
    <article>
        <h2 class="post_title post_detail"><a href="././multi_head_20240311.html" rel="bookmark" title="Permalink to 手寫Multi-Head Attention：從公式到代碼實現">手寫Multi-Head Attention：從公式到代碼實現</a></h2>
        <div class="entry-content blog-post">
            <div>
<div class="page-columns page-rows-contents page-layout-article" id="quarto-content">
<div class="sidebar margin-sidebar" id="quarto-margin-sidebar">
<nav class="toc-active" id="TOC" role="doc-toc">
<h2 id="toc-title">目录</h2>
<ul>
<li><a class="nav-link active" data-scroll-target="#scaled-dot-product-attention" href="#scaled-dot-product-attention" id="toc-scaled-dot-product-attention"><span class="toc-section-number">1</span>  Scaled Dot-product Attention</a>
<ul class="collapse">
<li><a class="nav-link" data-scroll-target="#attention的縮放係數" href="#attention的縮放係數" id="toc-attention的縮放係數"><span class="toc-section-number">1.1</span>  Attention的縮放係數</a></li>
<li><a class="nav-link" data-scroll-target="#代碼實現" href="#代碼實現" id="toc-代碼實現"><span class="toc-section-number">1.2</span>  代碼實現</a></li>
</ul></li>
<li><a class="nav-link" data-scroll-target="#multi-head-attention" href="#multi-head-attention" id="toc-multi-head-attention"><span class="toc-section-number">2</span>  Multi-head Attention</a></li>
<li><a class="nav-link" data-scroll-target="#練習題" href="#練習題" id="toc-練習題"><span class="toc-section-number">3</span>  練習題</a></li>
<li><a class="nav-link" data-scroll-target="#總結" href="#總結" id="toc-總結"><span class="toc-section-number">4</span>  總結</a></li>
</ul>
</nav>
</div>
<main class="content" id="quarto-document-content">
<header class="quarto-title-block default" id="title-block-header">
<div class="quarto-title">

</div>
<div>
<div class="description">
    本文介紹了應用廣泛、影響深遠的注意力機制，介紹了注意力、多頭注意力的特點，詳細解釋了注意力矩陣的縮放係數，給出了對應的Python實現和完整的驗證程序。注意力機制不僅是Transformer模型的基本模塊，其在計算機視覺、3D點雲處理等方向也有應用潛力，值得我們深入理解其中的細節，思考它的優點和局限。
  </div>
</div>
<div class="quarto-title-meta">
<div>
<div class="quarto-title-meta-heading">发布日期</div>
<div class="quarto-title-meta-contents">
<p class="date">2024年3月11日</p>
</div>
</div>
</div>
</header>
<p>自從“Attention is All You need”<span class="citation" data-cites="vaswani_attention_2017"><sup>[<a href="#ref-vaswani_attention_2017" role="doc-biblioref">1</a>]</sup></span>這篇文章發佈之後，注意力機制開始廣為人知。雖然一開始注意力機制被應用於自然語言處理領域，但人們很快發現它也能夠用於處理圖像、點雲等數據結構，並且取得非常好的效果。</p>
<p>本文介紹如何用pytorch實現文章<span class="citation" data-cites="vaswani_attention_2017"><sup>[<a href="#ref-vaswani_attention_2017" role="doc-biblioref">1</a>]</sup></span>提出的multi-head attention（MHA）。MHA是scaled dot-product attention（SDPA）<span class="citation" data-cites="vaswani_attention_2017 Bahdanau2014NeuralMT"><sup>[<a href="#ref-vaswani_attention_2017" role="doc-biblioref">1</a>,<a href="#ref-Bahdanau2014NeuralMT" role="doc-biblioref">2</a>]</sup></span>的改進。既然MHA是SDPA的multi-head版，那麼也許將SDPA稱為單頭注意力會比較形象。讓我們先從單頭注意力開始。</p>
<section class="level2" data-number="1" id="scaled-dot-product-attention">
<h2 class="anchored" data-anchor-id="scaled-dot-product-attention" data-number="1"><span class="header-section-number">1</span> Scaled Dot-product Attention</h2>
<p>SDPA的計算公式如下： <span id="eq-sdpa"><span class="math display">\[
\text{Attention}(Q, K, V) = \text{Softmax}(\frac{QK^T}{\sqrt{d_k}})V
\tag{1}\]</span></span> 其中，<span class="math inline">\(Q\)</span>、<span class="math inline">\(K\)</span>和<span class="math inline">\(V\)</span>分別為輸入的query、key和value向量。<span class="math inline">\(Q\)</span>的尺寸為<span class="math inline">\(L\times E_q\)</span>，<span class="math inline">\(K\)</span>的尺寸為<span class="math inline">\(S\times E_q\)</span>，<span class="math inline">\(V\)</span>的尺寸為<span class="math inline">\(S\times E_v\)</span>。</p>
<p><a href="#eq-sdpa">公式 1</a>中，<span class="math inline">\(QK^T\)</span>的計算可以理解為求query向量和key向量間的“匹配程度”，使得SDPA能夠根據匹配程度從value中取得相關的數據。Softmax函數在這裡起到歸一化的作用，使得SDPA的計算結果可以視為V的加權平均。</p>
<p>注意力機制是一個有趣的設計。RNN（Recurrent Neural Networks）結構面臨著長程記憶困難的問題；類似的，CNN（Convolutional Neural Networks）中也存在遠距離像素感知困難的問題。SDPA的特點是，為每一對query和key都作計算，而不考慮其時間或空間上的位置差距。</p>
<p>我個人的觀察是，注意力機制就好像構造了一個“完全二部圖”。這個二部圖由<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>兩個頂點集合組成，其中<span class="math inline">\(A\)</span>集合代表query，<span class="math inline">\(B\)</span>集合代表key和value。<span class="math inline">\(A\)</span>和<span class="math inline">\(B\)</span>之間的結點兩兩之間都有一條邊，邊的權重由query和key的“匹配程度”決定。類似SDPA，全連接層或卷積層也可以表示為二部圖，只不過其邊的權重是通過訓練得到的，固定的值，而在注意力機制中，邊的權重是動態的，根據query和key計算得到的。</p>
<section class="level3" data-number="1.1" id="attention的縮放係數">
<h3 class="anchored" data-anchor-id="attention的縮放係數" data-number="1.1"><span class="header-section-number">1.1</span> Attention的縮放係數</h3>
<p>從<a href="#eq-sdpa">公式 1</a>可以看到，在執行Softmax函數前，SDPA使用係數<span class="math inline">\(\frac{1}{\sqrt{d_k}}\)</span>對輸入作了縮放。<strong>為什麼是<span class="math inline">\(\sqrt{d_k}\)</span>，而不是其它係數呢？</strong>本節主要摘錄<a href="https://spaces.ac.cn/archives/9812">蘇劍林的系列文章</a>中的結論，對這個問題作簡要的討論。</p>
<p>Softmax是一個將向量映射為向量的函數。首先，我們注意到Softmax輸入向量的方差對於其輸出向量的分佈有重要影響。</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a aria-hidden="true" href="#cb1-1" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np </span>
<span id="cb1-2"><a aria-hidden="true" href="#cb1-2" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb1-3"><a aria-hidden="true" href="#cb1-3" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F </span>
<span id="cb1-4"><a aria-hidden="true" href="#cb1-4" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt </span>
<span id="cb1-5"><a aria-hidden="true" href="#cb1-5" tabindex="-1"></a>np.random.seed(<span class="dv">0</span>)</span>
<span id="cb1-6"><a aria-hidden="true" href="#cb1-6" tabindex="-1"></a></span>
<span id="cb1-7"><a aria-hidden="true" href="#cb1-7" tabindex="-1"></a>length <span class="op">=</span> <span class="dv">32</span></span>
<span id="cb1-8"><a aria-hidden="true" href="#cb1-8" tabindex="-1"></a>samples <span class="op">=</span> []</span>
<span id="cb1-9"><a aria-hidden="true" href="#cb1-9" tabindex="-1"></a>n_samples <span class="op">=</span> <span class="dv">32</span> </span>
<span id="cb1-10"><a aria-hidden="true" href="#cb1-10" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(n_samples): </span>
<span id="cb1-11"><a aria-hidden="true" href="#cb1-11" tabindex="-1"></a>    m <span class="op">=</span> i <span class="op">/</span> n_samples <span class="op">*</span> <span class="dv">10</span></span>
<span id="cb1-12"><a aria-hidden="true" href="#cb1-12" tabindex="-1"></a>    x <span class="op">=</span> np.random.randn(length) <span class="op">*</span> m </span>
<span id="cb1-13"><a aria-hidden="true" href="#cb1-13" tabindex="-1"></a>    y <span class="op">=</span> np.array(torch.softmax(torch.tensor(x), dim<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb1-14"><a aria-hidden="true" href="#cb1-14" tabindex="-1"></a>    samples.append((<span class="bu">sorted</span>(y), np.var(x)))</span>
<span id="cb1-15"><a aria-hidden="true" href="#cb1-15" tabindex="-1"></a>samples.sort(key<span class="op">=</span><span class="kw">lambda</span> x: x[<span class="dv">1</span>])  <span class="co"># 按方差排序</span></span>
<span id="cb1-16"><a aria-hidden="true" href="#cb1-16" tabindex="-1"></a>fig <span class="op">=</span> plt.figure()</span>
<span id="cb1-17"><a aria-hidden="true" href="#cb1-17" tabindex="-1"></a>ax <span class="op">=</span> plt.gca()</span>
<span id="cb1-18"><a aria-hidden="true" href="#cb1-18" tabindex="-1"></a>cax <span class="op">=</span> ax.matshow(np.array([s <span class="cf">for</span> s, i <span class="kw">in</span> samples]).transpose())</span>
<span id="cb1-19"><a aria-hidden="true" href="#cb1-19" tabindex="-1"></a>ax.set_xticklabels([<span class="ss">f'</span><span class="sc">{</span>i<span class="sc">:.2f}</span><span class="ss">'</span> <span class="cf">for</span> s, i <span class="kw">in</span> samples])</span>
<span id="cb1-20"><a aria-hidden="true" href="#cb1-20" tabindex="-1"></a>fig.colorbar(cax)<span class="op">;</span></span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center anchored" id="fig-softmax-output">
<figure class="figure">
<p><img class="figure-img" height="415" src="multi_head_20240311/figure-html/fig-softmax-output-output-1.png" width="485"/></p>
<p></p><figcaption class="figure-caption">图 1: Softmax輸入向量的方差對輸出向量的影響。橫軸表示輸入向量的方差。圖像的每一列表示Softmax對應該方差時輸出的向量。為便於觀察，向量都經過排序。</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>由<a href="#fig-softmax-output">圖 1</a>可見，Softmax函數的輸入向量方差越小，則輸出越接近<span class="math inline">\(\vec 0\)</span>；反之，Softmax的輸出越接近one-hot編碼，即只有一個元素是1，其它元素都接近0. 然而，這兩種極端情況都容易引發梯度消失問題。（這在我的<a href="./softmax20240109_zh-cn.html">《Softmax函數的性質》</a>一文中已經給出了推導。）</p>
<p>為了避免梯度消失，我們既不希望Softmax的輸出接近<span class="math inline">\(\vec 0\)</span>，也不希望其成為one-hot編碼。下面的程序通過直觀的方式演示了其中的原因：</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a aria-hidden="true" href="#cb2-1" tabindex="-1"></a><span class="co"># 向量的長度</span></span>
<span id="cb2-2"><a aria-hidden="true" href="#cb2-2" tabindex="-1"></a>length <span class="op">=</span> <span class="dv">128</span>  </span>
<span id="cb2-3"><a aria-hidden="true" href="#cb2-3" tabindex="-1"></a><span class="co"># 縮放係數，影響輸入向量的方差大小</span></span>
<span id="cb2-4"><a aria-hidden="true" href="#cb2-4" tabindex="-1"></a>scales <span class="op">=</span> [<span class="dv">2</span><span class="op">**</span>(i <span class="op">/</span> <span class="dv">3</span>) <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>)]</span>
<span id="cb2-5"><a aria-hidden="true" href="#cb2-5" tabindex="-1"></a>xs <span class="op">=</span> []</span>
<span id="cb2-6"><a aria-hidden="true" href="#cb2-6" tabindex="-1"></a>ys <span class="op">=</span> []</span>
<span id="cb2-7"><a aria-hidden="true" href="#cb2-7" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">1000</span>): </span>
<span id="cb2-8"><a aria-hidden="true" href="#cb2-8" tabindex="-1"></a>    <span class="co"># 隨機選擇一個縮放係數</span></span>
<span id="cb2-9"><a aria-hidden="true" href="#cb2-9" tabindex="-1"></a>    m <span class="op">=</span> np.random.choice(scales)</span>
<span id="cb2-10"><a aria-hidden="true" href="#cb2-10" tabindex="-1"></a>    <span class="co"># 隨機初始化Softmax的輸入，並縮放 </span></span>
<span id="cb2-11"><a aria-hidden="true" href="#cb2-11" tabindex="-1"></a>    x <span class="op">=</span> torch.tensor(np.random.randn(length) <span class="op">*</span> m, requires_grad<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb2-12"><a aria-hidden="true" href="#cb2-12" tabindex="-1"></a>    <span class="co"># 執行Softmax函數</span></span>
<span id="cb2-13"><a aria-hidden="true" href="#cb2-13" tabindex="-1"></a>    y <span class="op">=</span> torch.softmax(x, dim<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb2-14"><a aria-hidden="true" href="#cb2-14" tabindex="-1"></a>    <span class="co"># 橫軸為max(y)</span></span>
<span id="cb2-15"><a aria-hidden="true" href="#cb2-15" tabindex="-1"></a>    xs.append(y.<span class="bu">max</span>().item())</span>
<span id="cb2-16"><a aria-hidden="true" href="#cb2-16" tabindex="-1"></a>    label <span class="op">=</span> torch.rand(y.shape)</span>
<span id="cb2-17"><a aria-hidden="true" href="#cb2-17" tabindex="-1"></a>    loss <span class="op">=</span> torch.<span class="bu">abs</span>(label <span class="op">-</span> y).mean()</span>
<span id="cb2-18"><a aria-hidden="true" href="#cb2-18" tabindex="-1"></a>    loss.backward()</span>
<span id="cb2-19"><a aria-hidden="true" href="#cb2-19" tabindex="-1"></a>    <span class="co"># 縱軸為梯度絕對值的最大值</span></span>
<span id="cb2-20"><a aria-hidden="true" href="#cb2-20" tabindex="-1"></a>    ys.append(x.grad.<span class="bu">abs</span>().<span class="bu">max</span>().item())</span>
<span id="cb2-21"><a aria-hidden="true" href="#cb2-21" tabindex="-1"></a>plt.scatter(xs, ys)</span>
<span id="cb2-22"><a aria-hidden="true" href="#cb2-22" tabindex="-1"></a>plt.xlabel(<span class="st">'Softmax輸出的最大值'</span>)</span>
<span id="cb2-23"><a aria-hidden="true" href="#cb2-23" tabindex="-1"></a>plt.ylabel(<span class="st">'梯度絕對值的最大值'</span>)</span>
<span id="cb2-24"><a aria-hidden="true" href="#cb2-24" tabindex="-1"></a>plt.show()</span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div class="quarto-figure quarto-figure-center anchored" id="fig-softmax-grad">
<figure class="figure">
<p><img class="figure-img" height="427" src="multi_head_20240311/figure-html/fig-softmax-grad-output-1.png" width="607"/></p>
<p></p><figcaption class="figure-caption">图 2: 圖像的橫軸為Softmax的輸出最大值。當該數值接近最小值的時候，表示輸出向量接近均勻分佈。而當該數值接近1的時候，表明輸出向量接近one-hot向量；縱軸是經過損失的反向傳播後，梯度絕對值的最大值。</figcaption><p></p>
</figure>
</div>
</div>
</div>
<p>從<a href="#fig-softmax-grad">圖 2</a>可以看到，當Softmax的輸入接近<span class="math inline">\(0\)</span>向量或者one-hot向量的時候，容易發生梯度消失現象，此時梯度的絕對值接近<span class="math inline">\(0\)</span>.</p>
綜上所述，為盡量避免Softmax的梯度消失，<strong>控制輸入向量的方差很重要</strong>。假設<span class="math inline">\(\vec q\)</span>和<span class="math inline">\(\vec k\)</span>都是<span class="math inline">\(0\)</span>均值的，二者相互獨立，方差分別為<span class="math inline">\(v_q\)</span>和<span class="math inline">\(v_k\)</span>，不難證明它們的內積<span class="math inline">\(\vec q^T \vec k\)</span>的數學期望為<span class="math inline">\(0\)</span>。
<details>
<summary>
展開證明
</summary>
<span class="math display">\[
\begin{align*}
\mathbb{E}(\vec q^T \vec k) &amp;= \mathbb{E}\left(\sum_i q_i k_i\right) \\
&amp;= \sum_i \mathbb{E}(q_i k_i) \\
&amp;= \sum_i \mathbb{E}(q_i) \mathbb{E}(k_i) \\
&amp;= \sum_i 0 \cdot 0 \\
&amp;= 0.
\end{align*}
\]</span>
</details>
<span class="math inline">\(\vec q^T \vec k\)</span>的方差為<span class="math inline">\(d_k v_q v_k\)</span>。
<details>
<summary>
展開證明
</summary>
<span class="math display">\[
\begin{align*}
\text{Var}(\vec q^T \vec k) &amp;= \text{Var}\left(\sum_{i=1}^{d_k} (q_i k_i)\right) \\
\end{align*}
\]</span> 假設<span class="math inline">\(\vec q^T \vec k\)</span>的任意兩個維度都是獨立同分佈的，那麼 <span class="math display">\[
\begin{align*} \\
\text{Var}(\vec q^T \vec k) &amp;= d_k Var(q_i k_i) \\
&amp;= d_k (\mathbb E(q_i^2 k_i^2) - (\mathbb E(q_i k_i))^2)\\
&amp;= d_k (\mathbb E(q_i^2)\mathbb E(k_i^2) - (\mathbb E(q_i)
\mathbb E(k_i))^2)\\
&amp;= d_k (\mathbb E(q_i^2)\mathbb E(k_i^2) )\\
&amp;= d_k \left( Var(q_i) Var(k_i) \right) \\
&amp;= d_k v_q v_k
\end{align*}
\]</span> 於是<span class="math inline">\({\rm Var} (\vec q^T \vec k/\sqrt{d_k})=v_q v_k\)</span>。
</details>
<p>因此，假設輸入的query和value是標準正態分佈（均值為<span class="math inline">\(0\)</span>，方差為<span class="math inline">\(1\)</span>），那麼經過<span class="math inline">\(1/\sqrt{d_k}\)</span>的縮放處理，就能保持softmax的輸入也是標準正態分佈，避免方差過小或過大。</p>
<!-- ```{python}
import numpy as np
def test(var, length=1024, d_k=64):
    q = np.random.randn(length, d_k) * np.sqrt(var)
    k = np.random.randn(length, d_k) * np.sqrt(var)
    qk = (q * k).sum(-1)
    var_qk = np.std(qk)**2 
    print('{:.2f};{:.2f};{:.2f}'.format(var, var_qk, var**2 * d_k))

test(1); test(2); test(4); test(8)
test(16); test(32); test(64); test(128)
``` -->
</section>
<section class="level3" data-number="1.2" id="代碼實現">
<h3 class="anchored" data-anchor-id="代碼實現" data-number="1.2"><span class="header-section-number">1.2</span> 代碼實現</h3>
<p>在充分理解清楚SDPA的計算公式的前提下，用pytorch實現它就不困難。需要注意的是<a href="#eq-sdpa">公式 1</a>中沒有體現attention mask，但我們要在代碼實現中支持它。attention mask用於在訓練中阻止模型到未來的token，這對訓練聊天機器人、機器翻譯模型是等生成式模型是必要的。</p>
<p>在實現中，<code>mask</code>矩陣的值會被加到注意力矩陣上。如果<code>mask</code>矩陣中有一個值非常小（接近<span class="math inline">\(-\infty\)</span>），那麼對應的注意力就會在<code>softmax</code>中被抑制。</p>
<p>下列代碼實現了scaled dot-product attention：</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a aria-hidden="true" href="#cb3-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a aria-hidden="true" href="#cb3-2" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn  </span>
<span id="cb3-3"><a aria-hidden="true" href="#cb3-3" tabindex="-1"></a><span class="im">import</span> torch.nn.functional <span class="im">as</span> F</span>
<span id="cb3-4"><a aria-hidden="true" href="#cb3-4" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb3-5"><a aria-hidden="true" href="#cb3-5" tabindex="-1"></a></span>
<span id="cb3-6"><a aria-hidden="true" href="#cb3-6" tabindex="-1"></a><span class="kw">def</span> my_scaled_dot_product_attention(q, k, v, mask<span class="op">=</span><span class="va">None</span>):    </span>
<span id="cb3-7"><a aria-hidden="true" href="#cb3-7" tabindex="-1"></a>    embed_dim <span class="op">=</span> q.shape[<span class="op">-</span><span class="dv">1</span>]</span>
<span id="cb3-8"><a aria-hidden="true" href="#cb3-8" tabindex="-1"></a>    attn_weights <span class="op">=</span> torch.einsum(<span class="st">'nle,nse-&gt;nls'</span>, q, k)</span>
<span id="cb3-9"><a aria-hidden="true" href="#cb3-9" tabindex="-1"></a>    attn_weights <span class="op">/=</span> np.sqrt(embed_dim)</span>
<span id="cb3-10"><a aria-hidden="true" href="#cb3-10" tabindex="-1"></a>    <span class="cf">if</span> mask <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>: attn_weights <span class="op">+=</span> mask </span>
<span id="cb3-11"><a aria-hidden="true" href="#cb3-11" tabindex="-1"></a>    attn_weights <span class="op">=</span> torch.softmax(attn_weights, dim<span class="op">=-</span><span class="dv">1</span>) </span>
<span id="cb3-12"><a aria-hidden="true" href="#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a aria-hidden="true" href="#cb3-13" tabindex="-1"></a>    ret <span class="op">=</span> torch.einsum(<span class="st">'nls,nse-&gt;nle'</span>, attn_weights, v)</span>
<span id="cb3-14"><a aria-hidden="true" href="#cb3-14" tabindex="-1"></a>    <span class="cf">return</span> ret, attn_weights</span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
</div>
<p>讓我們看看實現的<code>my_scaled_dot_product_attention</code>函數能不能正常運行：</p>
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a aria-hidden="true" href="#cb4-1" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">1</span> </span>
<span id="cb4-2"><a aria-hidden="true" href="#cb4-2" tabindex="-1"></a>length <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb4-3"><a aria-hidden="true" href="#cb4-3" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">4</span> </span>
<span id="cb4-4"><a aria-hidden="true" href="#cb4-4" tabindex="-1"></a>q <span class="op">=</span> torch.rand((batch_size, length, embed_dim))</span>
<span id="cb4-5"><a aria-hidden="true" href="#cb4-5" tabindex="-1"></a>k <span class="op">=</span> torch.rand((batch_size, length, embed_dim))</span>
<span id="cb4-6"><a aria-hidden="true" href="#cb4-6" tabindex="-1"></a>v <span class="op">=</span> torch.rand((batch_size, length, embed_dim))</span>
<span id="cb4-7"><a aria-hidden="true" href="#cb4-7" tabindex="-1"></a>mask <span class="op">=</span> torch.rand((length, length))</span>
<span id="cb4-8"><a aria-hidden="true" href="#cb4-8" tabindex="-1"></a>ret, attn_weights <span class="op">=</span> my_scaled_dot_product_attention(q, k, v, mask)</span>
<span id="cb4-9"><a aria-hidden="true" href="#cb4-9" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'attention操作返回結果：'</span>, ret)</span>
<span id="cb4-10"><a aria-hidden="true" href="#cb4-10" tabindex="-1"></a><span class="bu">print</span>(<span class="st">'attention矩陣為：'</span>, attn_weights)</span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>attention操作返回結果： tensor([[[0.5092, 0.4412, 0.1042, 0.1914],
         [0.6837, 0.5136, 0.1024, 0.2412]]])
attention矩陣為： tensor([[[0.5479, 0.4521],
         [0.3397, 0.6603]]])</code></pre>
</div>
</div>
<p>一個檢查實現正確性的小技巧是，觀察返回attention矩陣的各列之和是否為1. 如果不為1，說明實現有問題。</p>
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a aria-hidden="true" href="#cb6-1" tabindex="-1"></a><span class="bu">print</span>(attn_weights.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>))</span>
<span id="cb6-2"><a aria-hidden="true" href="#cb6-2" tabindex="-1"></a><span class="cf">assert</span> torch.<span class="bu">all</span>(attn_weights.<span class="bu">sum</span>(<span class="op">-</span><span class="dv">1</span>) <span class="op">-</span> <span class="dv">1</span> <span class="op">&lt;</span> <span class="fl">1e-6</span>)</span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>tensor([[1., 1.]])</code></pre>
</div>
</div>
<p>在準備好scaled dot product attention的基礎上，我們便可以著手實現multi-head attention了。</p>
</section>
</section>
<section class="level2" data-number="2" id="multi-head-attention">
<h2 class="anchored" data-anchor-id="multi-head-attention" data-number="2"><span class="header-section-number">2</span> Multi-head Attention</h2>
<p>根據原論文描述，MHA的實現為： <span class="math display">\[\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, ..., \text{head}_h)W^O \]</span> 其中<span class="math inline">\(\text{head}_i = \text{Attention}(QW^Q_i , KW^K_i ,VW^V_i)\)</span>. 據此可知，MHA操作為：</p>
<ol type="1">
<li>將query、key和value拆分成若干個head，對這些head分別作線性變換；</li>
<li>對每個head分別執行scaled dot-product attention；</li>
<li>將所有head的attention的結果合併，經過一層線性變換後返回。</li>
</ol>
<p>通過在多個head上執行attention，MHA允許模型在不同的子空間收集不同位置的信息，增加了attention機制的表達能力。</p>
<p>代碼實現如下：</p>
<div class="cell" data-execution_count="7">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a aria-hidden="true" href="#cb8-1" tabindex="-1"></a><span class="kw">class</span> MyMultiheadAttention(nn.Module):</span>
<span id="cb8-2"><a aria-hidden="true" href="#cb8-2" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(</span>
<span id="cb8-3"><a aria-hidden="true" href="#cb8-3" tabindex="-1"></a>        <span class="va">self</span>,</span>
<span id="cb8-4"><a aria-hidden="true" href="#cb8-4" tabindex="-1"></a>        embed_dim,</span>
<span id="cb8-5"><a aria-hidden="true" href="#cb8-5" tabindex="-1"></a>        num_heads,</span>
<span id="cb8-6"><a aria-hidden="true" href="#cb8-6" tabindex="-1"></a>    ):</span>
<span id="cb8-7"><a aria-hidden="true" href="#cb8-7" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb8-8"><a aria-hidden="true" href="#cb8-8" tabindex="-1"></a>        <span class="va">self</span>.embed_dim <span class="op">=</span> embed_dim </span>
<span id="cb8-9"><a aria-hidden="true" href="#cb8-9" tabindex="-1"></a>        <span class="va">self</span>.num_heads <span class="op">=</span> num_heads </span>
<span id="cb8-10"><a aria-hidden="true" href="#cb8-10" tabindex="-1"></a>        <span class="va">self</span>.in_proj_weight <span class="op">=</span> nn.Parameter(</span>
<span id="cb8-11"><a aria-hidden="true" href="#cb8-11" tabindex="-1"></a>            torch.rand((<span class="dv">3</span> <span class="op">*</span> embed_dim, embed_dim))</span>
<span id="cb8-12"><a aria-hidden="true" href="#cb8-12" tabindex="-1"></a>        )</span>
<span id="cb8-13"><a aria-hidden="true" href="#cb8-13" tabindex="-1"></a>        <span class="va">self</span>.in_proj_bias <span class="op">=</span> nn.Parameter(</span>
<span id="cb8-14"><a aria-hidden="true" href="#cb8-14" tabindex="-1"></a>            torch.rand((<span class="dv">3</span> <span class="op">*</span> embed_dim))</span>
<span id="cb8-15"><a aria-hidden="true" href="#cb8-15" tabindex="-1"></a>        )</span>
<span id="cb8-16"><a aria-hidden="true" href="#cb8-16" tabindex="-1"></a>        <span class="va">self</span>.out_proj <span class="op">=</span> nn.Linear(embed_dim, embed_dim)</span>
<span id="cb8-17"><a aria-hidden="true" href="#cb8-17" tabindex="-1"></a></span>
<span id="cb8-18"><a aria-hidden="true" href="#cb8-18" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, query, key, value, attn_mask<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb8-19"><a aria-hidden="true" href="#cb8-19" tabindex="-1"></a>        L, N, q_dim <span class="op">=</span> query.shape</span>
<span id="cb8-20"><a aria-hidden="true" href="#cb8-20" tabindex="-1"></a>        S, N, k_dim <span class="op">=</span> key.shape</span>
<span id="cb8-21"><a aria-hidden="true" href="#cb8-21" tabindex="-1"></a>        S, N, v_dim <span class="op">=</span> value.shape</span>
<span id="cb8-22"><a aria-hidden="true" href="#cb8-22" tabindex="-1"></a>        </span>
<span id="cb8-23"><a aria-hidden="true" href="#cb8-23" tabindex="-1"></a>        <span class="cf">assert</span> <span class="va">self</span>.embed_dim <span class="op">%</span> <span class="va">self</span>.num_heads <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb8-24"><a aria-hidden="true" href="#cb8-24" tabindex="-1"></a>        weight_query, weight_key, weight_value <span class="op">=</span> <span class="va">self</span>.in_proj_weight.chunk(<span class="dv">3</span>)</span>
<span id="cb8-25"><a aria-hidden="true" href="#cb8-25" tabindex="-1"></a>        bias_query, bias_key, bias_value <span class="op">=</span> <span class="va">self</span>.in_proj_bias.chunk(<span class="dv">3</span>)</span>
<span id="cb8-26"><a aria-hidden="true" href="#cb8-26" tabindex="-1"></a></span>
<span id="cb8-27"><a aria-hidden="true" href="#cb8-27" tabindex="-1"></a>        query <span class="op">=</span> F.linear(query, weight_query, bias_query)</span>
<span id="cb8-28"><a aria-hidden="true" href="#cb8-28" tabindex="-1"></a>        key <span class="op">=</span> F.linear(key, weight_key, bias_key)</span>
<span id="cb8-29"><a aria-hidden="true" href="#cb8-29" tabindex="-1"></a>        value <span class="op">=</span> F.linear(value, weight_value, bias_value)</span>
<span id="cb8-30"><a aria-hidden="true" href="#cb8-30" tabindex="-1"></a></span>
<span id="cb8-31"><a aria-hidden="true" href="#cb8-31" tabindex="-1"></a>        <span class="co"># L, N, E -&gt; N*n_heads, L, E/n_heads</span></span>
<span id="cb8-32"><a aria-hidden="true" href="#cb8-32" tabindex="-1"></a>        query <span class="op">=</span> query.view(L, N <span class="op">*</span> <span class="va">self</span>.num_heads, <span class="op">-</span><span class="dv">1</span>).permute(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb8-33"><a aria-hidden="true" href="#cb8-33" tabindex="-1"></a>        key <span class="op">=</span> key.view(S, N <span class="op">*</span> <span class="va">self</span>.num_heads, <span class="op">-</span><span class="dv">1</span>).permute(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb8-34"><a aria-hidden="true" href="#cb8-34" tabindex="-1"></a>        value <span class="op">=</span> value.view(S, N <span class="op">*</span> <span class="va">self</span>.num_heads, <span class="op">-</span><span class="dv">1</span>).permute(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>)</span>
<span id="cb8-35"><a aria-hidden="true" href="#cb8-35" tabindex="-1"></a></span>
<span id="cb8-36"><a aria-hidden="true" href="#cb8-36" tabindex="-1"></a>        out, attn_weights <span class="op">=</span> my_scaled_dot_product_attention(query, key, value, mask<span class="op">=</span>attn_mask)</span>
<span id="cb8-37"><a aria-hidden="true" href="#cb8-37" tabindex="-1"></a>        out <span class="op">=</span> out.permute(<span class="dv">1</span>, <span class="dv">0</span>, <span class="dv">2</span>).reshape(L, N, <span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb8-38"><a aria-hidden="true" href="#cb8-38" tabindex="-1"></a>        attn_weights <span class="op">=</span> attn_weights.reshape(N, <span class="op">-</span><span class="dv">1</span>, L, S).mean(<span class="dv">1</span>)</span>
<span id="cb8-39"><a aria-hidden="true" href="#cb8-39" tabindex="-1"></a></span>
<span id="cb8-40"><a aria-hidden="true" href="#cb8-40" tabindex="-1"></a>        out <span class="op">=</span> <span class="va">self</span>.out_proj(out)</span>
<span id="cb8-41"><a aria-hidden="true" href="#cb8-41" tabindex="-1"></a>        <span class="cf">return</span> out, attn_weights</span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
</div>
<p>首先，我們先比較一下本文實現的multi-head attention和torch官方的實現的參數設置。我們將會看到所有參數名稱和參數尺寸都是一致的。這表明我們的multi-head attention與torch的官方實現兼容。</p>
<div class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a aria-hidden="true" href="#cb9-1" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="dv">2</span></span>
<span id="cb9-2"><a aria-hidden="true" href="#cb9-2" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb9-3"><a aria-hidden="true" href="#cb9-3" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb9-4"><a aria-hidden="true" href="#cb9-4" tabindex="-1"></a>torch_mha <span class="op">=</span> nn.MultiheadAttention(embed_dim, num_heads)</span>
<span id="cb9-5"><a aria-hidden="true" href="#cb9-5" tabindex="-1"></a><span class="cf">for</span> n, p <span class="kw">in</span> torch_mha.named_parameters():</span>
<span id="cb9-6"><a aria-hidden="true" href="#cb9-6" tabindex="-1"></a>    <span class="bu">print</span>(n, p.shape)</span>
<span id="cb9-7"><a aria-hidden="true" href="#cb9-7" tabindex="-1"></a><span class="bu">print</span>(<span class="st">''</span>)</span>
<span id="cb9-8"><a aria-hidden="true" href="#cb9-8" tabindex="-1"></a></span>
<span id="cb9-9"><a aria-hidden="true" href="#cb9-9" tabindex="-1"></a>my_mha <span class="op">=</span> MyMultiheadAttention(embed_dim, num_heads)</span>
<span id="cb9-10"><a aria-hidden="true" href="#cb9-10" tabindex="-1"></a><span class="cf">for</span> n, p <span class="kw">in</span> my_mha.named_parameters():</span>
<span id="cb9-11"><a aria-hidden="true" href="#cb9-11" tabindex="-1"></a>    <span class="bu">print</span>(n, p.shape)</span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>in_proj_weight torch.Size([48, 16])
in_proj_bias torch.Size([48])
out_proj.weight torch.Size([16, 16])
out_proj.bias torch.Size([16])

in_proj_weight torch.Size([48, 16])
in_proj_bias torch.Size([48])
out_proj.weight torch.Size([16, 16])
out_proj.bias torch.Size([16])</code></pre>
</div>
</div>
<p>接下來，我們檢查我們實現的MHA和torch版本MHA的輸出是否完全一致。在進行比較之前，我們先進行參數的複製，保持兩個模塊的參數完全相同。</p>
<div class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a aria-hidden="true" href="#cb11-1" tabindex="-1"></a>my_mha_parameters <span class="op">=</span> <span class="bu">dict</span>(my_mha.named_parameters())</span>
<span id="cb11-2"><a aria-hidden="true" href="#cb11-2" tabindex="-1"></a><span class="cf">for</span> n, p <span class="kw">in</span> torch_mha.named_parameters():</span>
<span id="cb11-3"><a aria-hidden="true" href="#cb11-3" tabindex="-1"></a>    my_mha_parameters[n].data.copy_(p)</span>
<span id="cb11-4"><a aria-hidden="true" href="#cb11-4" tabindex="-1"></a>    <span class="cf">assert</span> torch.<span class="bu">all</span>(p <span class="op">==</span> my_mha_parameters[n])</span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
</div>
<p>下列代碼對函數的輸出進行檢查：</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a aria-hidden="true" href="#cb12-1" tabindex="-1"></a>query <span class="op">=</span> torch.rand((length, batch_size, embed_dim))</span>
<span id="cb12-2"><a aria-hidden="true" href="#cb12-2" tabindex="-1"></a>key <span class="op">=</span> torch.rand((length, batch_size, embed_dim)) </span>
<span id="cb12-3"><a aria-hidden="true" href="#cb12-3" tabindex="-1"></a>value <span class="op">=</span> torch.rand((length, batch_size, embed_dim))</span>
<span id="cb12-4"><a aria-hidden="true" href="#cb12-4" tabindex="-1"></a></span>
<span id="cb12-5"><a aria-hidden="true" href="#cb12-5" tabindex="-1"></a><span class="co"># prepare mask </span></span>
<span id="cb12-6"><a aria-hidden="true" href="#cb12-6" tabindex="-1"></a>attn_bias <span class="op">=</span> torch.zeros(length, length, dtype<span class="op">=</span>query.dtype)</span>
<span id="cb12-7"><a aria-hidden="true" href="#cb12-7" tabindex="-1"></a>temp_mask <span class="op">=</span> torch.ones(length, length, dtype<span class="op">=</span>torch.<span class="bu">bool</span>).tril(diagonal<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb12-8"><a aria-hidden="true" href="#cb12-8" tabindex="-1"></a>attn_bias.masked_fill_(temp_mask.logical_not(), <span class="bu">float</span>(<span class="st">"-inf"</span>))</span>
<span id="cb12-9"><a aria-hidden="true" href="#cb12-9" tabindex="-1"></a>attn_bias.to(query.dtype)</span>
<span id="cb12-10"><a aria-hidden="true" href="#cb12-10" tabindex="-1"></a></span>
<span id="cb12-11"><a aria-hidden="true" href="#cb12-11" tabindex="-1"></a>ret, attn_weights <span class="op">=</span> my_mha(query, key, value, attn_mask<span class="op">=</span>attn_bias)</span>
<span id="cb12-12"><a aria-hidden="true" href="#cb12-12" tabindex="-1"></a>ret2, attn_weights2 <span class="op">=</span> torch_mha(query, key, value, need_weights<span class="op">=</span><span class="va">True</span>, attn_mask<span class="op">=</span>attn_bias)</span>
<span id="cb12-13"><a aria-hidden="true" href="#cb12-13" tabindex="-1"></a>error <span class="op">=</span> torch.mean(torch.<span class="bu">abs</span>(ret <span class="op">-</span> ret2)).item()</span>
<span id="cb12-14"><a aria-hidden="true" href="#cb12-14" tabindex="-1"></a>error2 <span class="op">=</span> torch.mean(torch.<span class="bu">abs</span>(attn_weights <span class="op">-</span> attn_weights2)).item()</span>
<span id="cb12-15"><a aria-hidden="true" href="#cb12-15" tabindex="-1"></a><span class="bu">print</span>(<span class="ss">f'Difference: </span><span class="sc">{</span>error<span class="sc">:.4f}</span><span class="ss">, </span><span class="sc">{</span>error2<span class="sc">:.4f}</span><span class="ss">'</span>)</span>
<span id="cb12-16"><a aria-hidden="true" href="#cb12-16" tabindex="-1"></a></span>
<span id="cb12-17"><a aria-hidden="true" href="#cb12-17" tabindex="-1"></a><span class="cf">assert</span> error <span class="op">&lt;</span> <span class="fl">1e-6</span> <span class="kw">and</span> error2 <span class="op">&lt;</span> <span class="fl">1e-6</span></span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Difference: 0.0000, 0.0000</code></pre>
</div>
</div>
<p>檢查結果表明我們實現的multi-head attention輸出與torch官方實現的完全一致，驗證了代碼的正確性。</p>
</section>
<section class="level2" data-number="3" id="練習題">
<h2 class="anchored" data-anchor-id="練習題" data-number="3"><span class="header-section-number">3</span> 練習題</h2>
<div class="theorem example" id="exm-mha-num-of-parameters">
<p><span class="theorem-title"><strong>例子 1 </strong></span>改變MHA的heads數會如何改變MHA的參數量？</p>
<details>
<summary>
參考答案
</summary>
<p>MHA的參數量與head的數量無關，這從代碼中也能很容易看出來。</p>
<p>實驗也驗證了這一點。</p>
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a aria-hidden="true" href="#cb14-1" tabindex="-1"></a>embed_dim <span class="op">=</span> <span class="dv">16</span></span>
<span id="cb14-2"><a aria-hidden="true" href="#cb14-2" tabindex="-1"></a>num_heads <span class="op">=</span> <span class="dv">4</span></span>
<span id="cb14-3"><a aria-hidden="true" href="#cb14-3" tabindex="-1"></a>mha1 <span class="op">=</span> nn.MultiheadAttention(embed_dim, num_heads)</span>
<span id="cb14-4"><a aria-hidden="true" href="#cb14-4" tabindex="-1"></a>mha2 <span class="op">=</span> nn.MultiheadAttention(embed_dim, num_heads <span class="op">*</span> <span class="dv">2</span>)</span>
<span id="cb14-5"><a aria-hidden="true" href="#cb14-5" tabindex="-1"></a></span>
<span id="cb14-6"><a aria-hidden="true" href="#cb14-6" tabindex="-1"></a><span class="kw">def</span> count_parameters(m):</span>
<span id="cb14-7"><a aria-hidden="true" href="#cb14-7" tabindex="-1"></a>    c <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-8"><a aria-hidden="true" href="#cb14-8" tabindex="-1"></a>    <span class="cf">for</span> p <span class="kw">in</span> m.parameters(): c <span class="op">+=</span> p.numel()</span>
<span id="cb14-9"><a aria-hidden="true" href="#cb14-9" tabindex="-1"></a>    <span class="cf">return</span> c </span>
<span id="cb14-10"><a aria-hidden="true" href="#cb14-10" tabindex="-1"></a></span>
<span id="cb14-11"><a aria-hidden="true" href="#cb14-11" tabindex="-1"></a><span class="bu">print</span>(count_parameters(mha1), count_parameters(mha2))</span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1088 1088</code></pre>
</div>
</div>
</details>
</div>
<div class="theorem example" id="exm-mha-time-complexity">
<p><span class="theorem-title"><strong>例子 2 </strong></span>在自注意力機制中，MHA的時間複雜度是多少？和head數有關嗎？</p>
<details>
<summary>
參考答案
</summary>
<p>首先，由<a href="#eq-sdpa">公式 1</a>可以看出，單頭注意力機制的時間複雜度是<span class="math inline">\(O(n^2d)\)</span>，其中<span class="math inline">\(n\)</span>為序列長度，<span class="math inline">\(d\)</span>為特征維度。</p>
<p>接下來考慮MHA的時間複雜度。MHA先對輸入向量作線性變換得到Q、K、V，這部分的時間複雜度為<span class="math inline">\(O(nd^2)\)</span>；然後這些特征被拆分為<span class="math inline">\(n\)</span>個head，分別做attention，這個部分的時間複雜度為<span class="math inline">\(O(n^2\frac{d}{h} * h)\)</span>，其中<span class="math inline">\(h\)</span>為head數。最後MHA再做一次線性變化，時間複雜度為<span class="math inline">\(O(nd^2)\)</span>.</p>
<p>因此，總的來看，MHA的時間複雜度為<span class="math inline">\(O(n^2d + nd^2)\)</span>，與head數無關。</p>
從stack overflow的這個網頁可以看到一些有趣的討論：<a href="https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model">https://stackoverflow.com/questions/65703260/computational-complexity-of-self-attention-in-the-transformer-model</a>
</details>
</div>
</section>
<section class="level2" data-number="4" id="總結">
<h2 class="anchored" data-anchor-id="總結" data-number="4"><span class="header-section-number">4</span> 總結</h2>
<p>推薦閱讀：</p>
<ul>
<li><a href="https://pytorch.org/docs/2.2/generated/torch.nn.functional.scaled_dot_product_attention.html#torch.nn.functional.scaled_dot_product_attention">torch官方的scaled dot-product attention文檔</a>中簡要介紹了一種scaled dot-product attention的python實現方式。</li>
<li>論文MQA<span class="citation" data-cites="Shazeer2019MQA"><sup>[<a href="#ref-Shazeer2019MQA" role="doc-biblioref">3</a>]</sup></span>中的Background一節對各類Attention有詳細介紹，推薦一讀。</li>
</ul>
</section>
<div class="default" id="quarto-appendix"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">参考</h2><div class="references csl-bib-body" id="refs" role="doc-bibliography">
<div class="csl-entry" id="ref-vaswani_attention_2017" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">VASWANI A, SHAZEER N, PARMAR N, 等. <a href="http://arxiv.org/abs/1706.03762">Attention <span>Is</span> <span>All</span> <span>You</span> <span>Need</span></a>[J]. arXiv:1706.03762 [cs], 2017.</div>
</div>
<div class="csl-entry" id="ref-Bahdanau2014NeuralMT" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">BAHDANAU D, CHO K, BENGIO Y. <a href="https://api.semanticscholar.org/CorpusID:11212020">Neural Machine Translation by Jointly Learning to Align and Translate</a>[J]. CoRR, 2014, abs/1409.0473.</div>
</div>
<div class="csl-entry" id="ref-Shazeer2019MQA" role="doc-biblioentry">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">SHAZEER N M. <a href="https://api.semanticscholar.org/CorpusID:207880429">Fast Transformer Decoding: One Write-Head is All You Need</a>[J]. ArXiv, 2019, abs/1911.02150.</div>
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "已复制");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "已复制");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
</div>
        </div>
        <div class="post_list">
            <span>By </span>
            <a href="./">@執迷</a>
            <span> in </span>
            <span class="post_category"><a href="./" rel="bookmark" title="Permalink to 深度學習">[ 深度學習 ]</a></span>
            <span class="post_date">2024-03-11</span>
            <div><span>Tags : </span>
                
                
                <span><a href="./">#注意力機制, </a></span>
                
                <span><a href="./">#多頭注意力, </a></span>
                
                <span><a href="./">#深度學習, </a></span>
                
                <span><a href="./">#自然語言處理, </a></span>
                
                <span><a href="./">#Multi-head attention, </a></span>
                
                
            </div>

            <div class="entry-social">
                <span class="twitter"><a target="_blank" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=400,width=700');return false;" title="Twitter" href="https://twitter.com/share?url=././multi_head_20240311.html&text=手寫Multi-Head Attention：從公式到代碼實現&via="><img src="./theme/images/icons/twitter-s.png"></a></span>

                <span class="gplus"><a target="_blank" title="Google +" href="https://plus.google.com/share?url=././multi_head_20240311.html&hl=fr" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=450,width=650');return false;"><img src="./theme/images/icons/google-s.png"></a></span>

                <span class="facebook"><a target="_blank" title="Facebook" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=500,width=700');return false;" href="https://www.facebook.com/sharer.php?u=././multi_head_20240311.html&t=手寫Multi-Head Attention：從公式到代碼實現"><img src="./theme/images/icons/facebook-s.png"></a></span>

                <a  target="_blank" title="Linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=././multi_head_20240311.html&title=手寫Multi-Head Attention：從公式到代碼實現" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=450,width=650');return false;"><img src="./theme/images/icons/linkedin-s.png"></a>

                <span class="mail"><a href="mailto:?subject=手寫Multi-Head Attention：從公式到代碼實現&amp;body=Viens découvrir un article à propos de [手寫Multi-Head Attention：從公式到代碼實現] sur le site de 執迷. ././multi_head_20240311.html" title="Share by Email" target="_blank"><img src="./theme/images/icons/mail-s.png"></a></span>
            </div>
        </div>
        
    </article>
</section>

  </article>

  <!-- Footer -->
  <footer>
    <p>
      Blog powered by <a href="http://getpelican.com/">Pelican</a>, 
      which takes great advantage of <a href="http://python.org">Python</a>.
      Theme <a href="https://github.com/parbhat/pelican-blue">Pelican-Blue</a> by <a href="https://parbhatpuri.com/">@parbhat</a>.
    </p>
  </footer>

  
  <!-- Analytics -->
  <script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'G-G3N739QVFZ']);
  _gaq.push(['_trackPageview']);
  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
  </script>
  

</body>
</html>