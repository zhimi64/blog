<!doctype html>
<html lang="" itemscope itemtype="http://schema.org/Person">
<head>

  
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-G3N739QVFZ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-G3N739QVFZ');
  </script>
  

  
  <script>
    window.va = window.va || function () { (window.vaq = window.vaq || []).push(arguments); };
  </script>
  <script defer src="./_vercel/insights/script.js"></script>
  

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  
  

<!-- Quarto -->
<link href="./theme/css/quarto/quarto.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script>
<script src="./theme/scripts/quarto/quarto.js"></script>


  <meta charset="utf-8">
  <!-- Site Meta Data -->
  <title>【论文精读】Linear Transformer</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="執迷">

  <link rel="shortcut icon" href="favicon.ico">

  <!-- schema.org -->
  <meta itemprop="name" content="執迷的博客">
  <meta itemprop="image" content="">
  <meta itemprop="description" content="">

  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,600,700' rel='stylesheet' type='text/css'>
  <!-- Style Meta Data -->
  <link rel="stylesheet" href="./theme/css/style.css" type="text/css" />
  <link rel="stylesheet" href="./theme/css/pygments.css" type="text/css" />

  <!-- Feed Meta Data -->
  
  
    <link href="././feeds/rss_zh-cn.xml" type="application/atom+xml" rel="alternate" title="執迷的博客 RSS Feed" />
  

  <!-- Twitter Feed -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="">
  <meta name="twitter:image" content="">
  
<meta name="twitter:creator" content="">
<meta name="twitter:url" content="././linear_transformer_20250215_zh-cn.html">
<meta name="twitter:title" content="執迷的博客 ~ 【论文精读】Linear Transformer">
<meta name="twitter:description" content="">

<!-- Facebook Meta Data -->
<meta property="og:title" content="執迷的博客 ~ 【论文精读】Linear Transformer" />
<meta property="og:description" content="" />
<meta property="og:image" content="" />


<!-- MathJax -->

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>




</head>

<body>
  <!-- Sidebar -->
  <aside>
    <!--<center><a href="."><img id="avatar" src=""></a></center>-->
    <h1>執迷的博客</h1>
    
      <p>做一個有趣的人！</p>
    
    <br>

    

    <nav class="nav">
      <ul class="list-bare">
      
        
          <li><a class="nav__link" href="./index_zh-cn.html">Blog</a></li>
        
         
        
          <li><a class="nav__link" href="././about_zh-cn.html">About</a></li>
        
         
      </ul>
    </nav>

    <p class="social">
      
      
      
      <a href="././feeds/rss_zh-cn.xml" rel="alternate">
        <img src="./theme/images/icons/rss.png"></a>
      
    </p>

    <!--
    
    -->
  </aside>

  <!-- Content -->
  <article>
    
<section id="content">
    <h2 class="post_title post_detail"><a href="././linear_transformer_20250215_zh-cn.html" rel="bookmark" title="Permalink to 【论文精读】Linear Transformer">【论文精读】Linear Transformer</a></h2>
    
    <p>
        <span id="busuanzi_container_page_pv">
            本文瀏覽次數<span id="busuanzi_value_page_pv"></span>
        </span>
    </p>
    
    <div class="entry-content blog-post">
        <div>
<div class="page-columns page-rows-contents page-layout-article" id="quarto-content">
<div class="sidebar margin-sidebar" id="quarto-margin-sidebar">
<nav class="toc-active" id="TOC" role="doc-toc">
<h2 id="toc-title">目录</h2>
<ul>
<li><a class="nav-link active" data-scroll-target="#前言" href="#前言" id="toc-前言"><span class="header-section-number">1</span> 前言</a></li>
<li><a class="nav-link" data-scroll-target="#transformer的结构" href="#transformer的结构" id="toc-transformer的结构"><span class="header-section-number">2</span> Transformer的结构</a></li>
<li><a class="nav-link" data-scroll-target="#線性注意力" href="#線性注意力" id="toc-線性注意力"><span class="header-section-number">3</span> 线性注意力</a></li>
<li><a class="nav-link" data-scroll-target="#因果掩膜" href="#因果掩膜" id="toc-因果掩膜"><span class="header-section-number">4</span> 因果掩膜</a></li>
<li><a class="nav-link" data-scroll-target="#梯度計算" href="#梯度計算" id="toc-梯度計算"><span class="header-section-number">5</span> 梯度计算</a></li>
<li><a class="nav-link" data-scroll-target="#訓練和推理" href="#訓練和推理" id="toc-訓練和推理"><span class="header-section-number">6</span> 训练和推理</a></li>
<li><a class="nav-link" data-scroll-target="#transformer模型是rnn模型的特例" href="#transformer模型是rnn模型的特例" id="toc-transformer模型是rnn模型的特例"><span class="header-section-number">7</span> Transformer模型是RNN模型的特例</a></li>
<li><a class="nav-link" data-scroll-target="#推薦閲讀" href="#推薦閲讀" id="toc-推薦閲讀"><span class="header-section-number">8</span> 推荐阅读</a></li>
</ul>
</nav>
</div>
<main class="content" id="quarto-document-content">
<header class="quarto-title-block default" id="title-block-header">
<div class="quarto-title">

</div>
<div class="quarto-title-meta">
<div>
<div class="quarto-title-meta-heading">发布于</div>
<div class="quarto-title-meta-contents">
<p class="date">2025年5月15日</p>
</div>
</div>
</div>
</header>
<section class="level2" data-number="1" id="前言">
<h2 class="anchored" data-anchor-id="前言" data-number="1"><span class="header-section-number">1</span> 前言</h2>
<p>Transformer模型相较于传统的RNN模型更适合并行训练，因此得以在NLP领域得到广泛应用。但是<span class="math inline">\(O(n^2)\)</span>空间和时间复杂度是导致Transformer模型消耗显存和算力的痛点。这是第一个问题。</p>
<p>有一个观点是“压缩即智能”。但是我们在Transformer上没有看到任何“记忆压缩”的影子。它只是将所有token的特征排列在那里，可供后续随时取用。这是第二个问题，也是我最近常常思考的问题。</p>
<p>从上面两点出发，我最近著手调研（或者也可以说是考古吧😀）和实验一些线性Transformer的工作。</p>
<p>本文是我阅读<em>Transformers are RNNs</em><span class="citation" data-cites="transformers_are_rnns"><sup>[<a href="#ref-transformers_are_rnns" role="doc-biblioref">1</a>]</sup></span>这篇文章的笔记。这篇文章有一些有意思的贡献：</p>
<ol type="1">
<li>从softmax attention这个特例推广到一般的衡量query和key相似度的方法；</li>
<li>提出线性Transformer，它既能像传统Transformer一样并行训练，也能像RNN模型那样，在推理阶段维持常数的空间复杂度和时间复杂度。</li>
</ol>
</section>
<section class="level2" data-number="2" id="transformer的结构">
<h2 class="anchored" data-anchor-id="transformer的结构" data-number="2"><span class="header-section-number">2</span> Transformer的结构</h2>
<p>Transformer模型<span class="math inline">\(T\)</span>可以看做是一个类型为<span class="math inline">\(\mathbb R^{N\times F}\to \mathbb R^{N\times F}\)</span>的函数，这里<span class="math inline">\(N\)</span>是序列长度，<span class="math inline">\(F\)</span>是特征向量的维度。</p>
<p>一个Transformer有L层，记第<span class="math inline">\(l\)</span>层为<span class="math inline">\(T_l\)</span>，其结构为 <span class="math display">\[
T_l(x) = f_l(A_l(x) + x),
\]</span> 其中<span class="math inline">\(A_l\)</span>是Attention模块，<span class="math inline">\(f_l\)</span>是Feed-forward模块。</p>
<p>在Attention模块<span class="math inline">\(A_l\)</span>中，<span class="math inline">\(x\)</span>先经过线性转换<span class="math inline">\(\mathbf W_Q\in \mathbb R^{F\times D},\mathbf  W_K\in \mathbb R^{F\times D},\mathbf  W_V\in \mathbb R^{F\times M}\)</span>，得到<span class="math inline">\(\mathbf Q, \mathbf K, \mathbf V\)</span>（即query、key和value向量）： <span class="math display">\[
\begin{aligned}
\mathbf Q &amp;= \mathbf x\mathbf  W_Q,\\
\mathbf K &amp;= \mathbf x \mathbf W_K,\\
\mathbf V &amp;= \mathbf x\mathbf  W_V.
\end{aligned}
\]</span> 接著，Attention模块的输出为 <span id="eq-attention"><span class="math display">\[
\mathbf A_l(x) = \mathbf V' = \text{softmax}\left(\frac{\mathbf Q\mathbf K^T}{\sqrt{D}}\right)V.
\tag{1}\]</span></span> 可以看到<span class="math inline">\(V'\)</span>实际上是<span class="math inline">\(V\)</span>的加权和，其中每个位置特征的权重由<span class="math inline">\(Q\)</span>和<span class="math inline">\(K\)</span>计算得到。</p>
<p><a class="quarto-xref" href="#eq-attention">公式 1</a>使用softmax函数计算权重，这实际是下面的这个函数的一个特例： <span id="eq-generalized-attention"><span class="math display">\[
\mathbf V'_i = \frac{\sum_{j=1}^N \text{sim}(\mathbf Q_i, \mathbf K_j)V_j}{\sum_{j=1}^N \text{sim}(\mathbf Q_i, \mathbf K_j)}.
\tag{2}\]</span></span> softmax函数等同于将上式中的<span class="math inline">\(\text{sim}\)</span>函数定义为<span class="math inline">\(\exp(\frac{\mathbf Q_i^T\mathbf K_j}{\sqrt D})\)</span></p>
</section>
<section class="level2" data-number="3" id="線性注意力">
<h2 class="anchored" data-anchor-id="線性注意力" data-number="3"><span class="header-section-number">3</span> 线性注意力</h2>
<p>假设对于某<span class="math inline">\(\text{sim}\)</span>函数，存在<span class="math inline">\(\phi\)</span>函数，使得 <span class="math display">\[
\text{sim}(\mathbf Q_i, \mathbf K_j) = \phi(\mathbf Q_i)^T \phi(\mathbf K_j)
\]</span> 这样我们就将Attention模块线性化了。 利用矩阵乘法的结合律，上式可以改写为： <span id="eq-linear-attention"><span class="math display">\[
\mathbf V'_i = \frac{
    \phi(\mathbf Q_i)^T \sum_{j=1}^N \phi(\mathbf K_j) \mathbf V_j^T
}{
    \phi(\mathbf Q_i)^T \sum_{j=1}^N \phi(\mathbf K_j)
}
\tag{3}\]</span></span> 由于query和key两两之间都需要计算相似度，softmax-attention<span class="citation" data-cites="vaswani_attention_2017"><sup>[<a href="#ref-vaswani_attention_2017" role="doc-biblioref">2</a>]</sup></span>的时间复杂度是<span class="math inline">\(O(N^2)\)</span>；其空间复杂度同理也是<span class="math inline">\(O(N^2)\)</span>，因为需要存储注意力矩阵用于后续的反向传播。而线性注意力的时间复杂度是<span class="math inline">\(O(N)\)</span>，空间复杂度也是<span class="math inline">\(O(N)\)</span>。由<a class="quarto-xref" href="#eq-linear-attention">公式 3</a>可见，<span class="math inline">\(\sum_{j=1}^N \phi(\mathbf K_j) \mathbf V_j^T\)</span>和<span class="math inline">\(\sum_{j=1}^N \phi(\mathbf K_j)\)</span>均可以在计算时可以被缓存和复用。</p>
<p>为了使<span class="math inline">\(\text{sim}(\cdot)\)</span>是非负的，文章取<span class="math inline">\(\phi(x)\)</span>为 <span class="math display">\[
\phi(x) = \text{elu}(x) + 1
\]</span></p>
</section>
<section class="level2" data-number="4" id="因果掩膜">
<h2 class="anchored" data-anchor-id="因果掩膜" data-number="4"><span class="header-section-number">4</span> 因果掩膜</h2>
<p>在训练自回归模型时，每个位置的token都不能受到未来token的影响。位置<span class="math inline">\(i\)</span>上的token受到位置<span class="math inline">\(j\)</span>上的token的影响，当且仅当<span class="math inline">\(j\leq i\)</span>. 这应用于线性Transformer，需要将<a class="quarto-xref" href="#eq-generalized-attention">公式 2</a>改为 <span id="eq-generalized-attention-with-causal-mask"><span class="math display">\[
\mathbf V'_i=\frac{\sum_{j=1}^i \text{sim}(\mathbf Q_i, \mathbf K_j)\mathbf V_j}{\sum_{j=1}^i \text{sim}(\mathbf Q_i,\mathbf  K_j)}.
\tag{4}\]</span></span> 类似前面的推理，上式可改写为 <span id="eq-linear-attention-with-causal-mask"><span class="math display">\[
\mathbf V'_i = \frac{
    \phi(\mathbf Q_i)^T \sum_{j=1}^i \phi(\mathbf K_j) \mathbf V_j^T
}{
    \phi(\mathbf Q_i)^T \sum_{j=1}^i \phi(\mathbf K_j)
},
\tag{5}\]</span></span> 引入变量<span class="math inline">\(S_i\)</span>和<span class="math inline">\(Z_i\)</span>如下： <span class="math display">\[
\begin{aligned}
\mathbf S_i &amp;= \sum_{j=1}^i \phi(\mathbf K_j) \mathbf V_j^T,\\
\mathbf Z_i &amp;= \sum_{j=1}^i \phi(\mathbf K_j).
\end{aligned}
\]</span> <a class="quarto-xref" href="#eq-linear-attention-with-causal-mask">公式 5</a>可以改写为 <span id="eq-linear-attention-with-memory"><span class="math display">\[
\mathbf V'_i = \frac{
    \phi(\mathbf Q_i)^T\mathbf  S_i
}{
    \phi(\mathbf Q_i)^T \mathbf Z_i
}.
\tag{6}\]</span></span> 注意<span class="math inline">\(\mathbf S_i,\mathbf  Z_i\)</span>可以由<span class="math inline">\(\mathbf S_{i-1},\mathbf  Z_{i-1}\)</span>计算得到，时间复杂度为常数。</p>
</section>
<section class="level2" data-number="5" id="梯度計算">
<h2 class="anchored" data-anchor-id="梯度計算" data-number="5"><span class="header-section-number">5</span> 梯度计算</h2>
<p>如果根据<a class="quarto-xref" href="#eq-linear-attention-with-memory">公式 6</a>实现简单的深度学习模型，那么为了梯度计算，我们需要缓存所有的<span class="math inline">\(S_i, Z_i\)</span>，这将带来很大的空间复杂度。文章提出将对<a class="quarto-xref" href="#eq-linear-attention-with-causal-mask">公式 5</a>分子的求导实现为一种基于累加和（cumulative sum）的方法，实现前向传播和反向传播时的线性时间复杂度和常数空间复杂度。</p>
<p><strong>为了简便，本节的推导假设<span class="math inline">\(\mathbf Q, \mathbf K\)</span>中已经包含了<span class="math inline">\(\phi\)</span>函数。</strong></p>
<p>假设<span class="math inline">\(\overline{\mathbf V}_i\)</span>是<a class="quarto-xref" href="#eq-linear-attention-with-causal-mask">公式 5</a>的分子，即 <span id="eq-linear-attention-numerator"><span class="math display">\[
\overline{\mathbf V}_i = \mathbf Q_i^T \sum_{j=1}^i \mathbf K_j \mathbf V_j^T.
\tag{7}\]</span></span> 设<span class="math inline">\(L\)</span>为损失，已知<span class="math inline">\(\overline{\mathbf V}_i\)</span>和<span class="math inline">\(L\)</span>，则<span class="math inline">\(L\)</span>对<span class="math inline">\(\mathbf Q, \mathbf K, \mathbf V\)</span>的梯度分别为 <span id="eq-grad-Q"><span class="math display">\[
\nabla_{\mathbf Q_i}L = (\nabla_{\overline{\mathbf V}_i} L)(\sum_{j=1}^i \mathbf K_j \mathbf V_j^T)^T,
\tag{8}\]</span></span> <span id="eq-grad-K"><span class="math display">\[
\nabla_{\mathbf K_i}L = \left(\sum_{j=i}^N\mathbf Q_j (\nabla_{\overline{\mathbf V}_j}L)^T\right)\mathbf V_i,
\tag{9}\]</span></span> <span id="eq-grad-V"><span class="math display">\[
\nabla_{\mathbf V_i}L = \left(\sum_{j=i}^N \mathbf Q_j (\nabla_{\overline{V}_j}L)^T\right)^T \mathbf K_i,
\tag{10}\]</span></span> 其中<span class="math inline">\(\mathbf Q\in\mathbb R^{N\times D},\mathbf K\in\mathbb R^{N\times D}, \mathbf V\in \mathbb R^{N\times M}\)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
注记
</div>
</div>
<div class="callout-body-container callout-body">
<p>文章只考虑了分子的梯度计算。分母、整个分式的梯度计算交给<code>torch</code>自动处理。</p>
</div>
</div>
<p>以下是详细推导：</p>
<p>首先我们考虑矩阵中每个元素的计算，将<a class="quarto-xref" href="#eq-linear-attention-numerator">公式 7</a>中的矩阵、向量记号去除，得到 <span class="math display">\[
\overline{V}_{ie} = \sum_{d=1}^D Q_{id} \sum_{j=1}^i K_{jd} V_{je} = \sum_{d=1}^D \sum_{j=1}^i Q_{id} K_{jd} V_{je}。
\]</span></p>
<p>于是对于任意的<span class="math inline">\(Q_{lt}\)</span>，我们可以推导出梯度为 <span id="eq-grad-q"><span class="math display">\[
\frac{\partial L}{\partial Q_{lt}} = \sum_{e=1}^M \frac{\partial L}{\partial \overline{V}_{le}}\frac{\partial \overline{V}_{le}}{\partial Q_{lt}} = \sum_{e=1}^M\frac{\partial L}{\partial \overline{V}{le}}(\sum_{j=1}^lK_{jt}V_{je}).
\tag{11}\]</span></span></p>
<p>将其整理成矩阵形式，得到<a class="quarto-xref" href="#eq-grad-Q">公式 8</a></p>
<p>在<a class="quarto-xref" href="#eq-grad-q">公式 11</a>中，我们利用了<span class="math inline">\(\overline{\mathbf V}_l\)</span>只受<span class="math inline">\(l\)</span>位置的query（即<span class="math inline">\(\mathbf Q_l\)</span>）影响的性质。query只影响当下，而每个key和value都会对未来的计算产生影响。对于key，其梯度的计算方式为： <span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial K_{lt}} &amp;= \sum_{e=1}^M\sum_{i=l}^N \frac{\partial L}{\partial \overline{V}_{ie}} \frac{\partial \overline{V}_{ie}}{\partial K_{lt}} \\
&amp;= \sum_{e=1}^M\sum_{i=l}^N \frac{\partial L}{\partial \overline{V}_{ie}} \frac{\partial(\sum_{d=1}^D\sum_{j=1}^i Q_{id}K_{jd}V_{je})}{\partial K_{lt}} \\
&amp;= \sum_{e=1}^M\sum_{i=l}^N \frac{\partial L}{\partial \overline{V}_{ie}} Q_{it} V_{le}
\end{aligned}
\]</span> 将其整理为矩阵形式，得到<a class="quarto-xref" href="#eq-grad-K">公式 9</a></p>
<p>类似的，对于value，其梯度的计算方式为： <span class="math display">\[
\begin{aligned}
\frac{\partial L}{\partial V_{lt}} &amp;= \sum_{e=1}^M \sum_{i=l}^N \frac{\partial L}{\partial \overline{V}_{ie}} \frac{\overline{V}_{ie}}{\partial V_{lt}} \\
&amp;= \sum_{e=1}^M \sum_{i=l}^N \frac{\partial L}{\partial \overline{V}_{ie}} \frac{\partial (\sum_{d=1}^D \sum_{j=1}^i Q_{id} K_{jd} V_{je})}{\partial V_{lt}}\\
&amp;= \sum_{i=l}^N \frac{\partial L}{\partial \overline{V}_{it}} \sum_{d=1}^D Q_{id}K_{ld}
\end{aligned}
\]</span> 将其整理为矩阵形式，得到<a class="quarto-xref" href="#eq-grad-V">公式 10</a></p>
</section>
<section class="level2" data-number="6" id="訓練和推理">
<h2 class="anchored" data-anchor-id="訓練和推理" data-number="6"><span class="header-section-number">6</span> 训练和推理</h2>
<p>训练时，完整的训练序列是已知的，这允许Transformer模型实现并行的训练；而受限于计算方式，传统的RNN模型一般难并行训练。Transformer模型的每一步推理的时间复杂度是不同的，随著上下文长度的增加而增加；而RNN模型的时间复杂度是固定的。</p>
<p>文章提出的线性Transformer结合了两者的优点。</p>
</section>
<section class="level2" data-number="7" id="transformer模型是rnn模型的特例">
<h2 class="anchored" data-anchor-id="transformer模型是rnn模型的特例" data-number="7"><span class="header-section-number">7</span> Transformer模型是RNN模型的特例</h2>
<p>从本文的讨论，我们可以明显的看出，带因果掩膜的Transformer模型可以视作是RNN模型的特例，即Transformer模型可以看做是一个能维护一个内部状态（<span class="math inline">\(\mathbf S_i\)</span>和<span class="math inline">\(\mathbf Z_i\)</span>），在每次获得新输入时更新内部状态的模型。</p>
</section>
<section class="level2" data-number="8" id="推薦閲讀">
<h2 class="anchored" data-anchor-id="推薦閲讀" data-number="8"><span class="header-section-number">8</span> 推荐阅读</h2>
<ul>
<li><a href="https://www.spaces.ac.cn/archives/7546">《线性Attention的探索：Attention必须有个Softmax吗？》</a>。苏剑林的文章，非常好的总结分享了各种各样的线性Attention，甚至提出了自己的新设计。</li>
</ul>
</section>
<div class="default" id="quarto-appendix"><section class="quarto-appendix-contents" id="quarto-bibliography" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">参考文献</h2><div class="references csl-bib-body" data-entry-spacing="0" id="refs" role="list">
<div class="csl-entry" id="ref-transformers_are_rnns" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">KATHAROPOULOS A, VYAS A, PAPPAS N, 等. Transformers are RNNs: fast autoregressive transformers with linear attention[C]//Proceedings of the 37th International Conference on Machine Learning. JMLR.org.</div>
</div>
<div class="csl-entry" id="ref-vaswani_attention_2017" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">VASWANI A, SHAZEER N, PARMAR N, 等. <a href="http://arxiv.org/abs/1706.03762">Attention <span>Is</span> <span>All</span> <span>You</span> <span>Need</span></a>[J]. arXiv:1706.03762 [cs], 2017.</div>
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "已复制");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "已复制");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
</div>
    </div>
    <div class="post_list">
        <span>By </span>
        <a href="./">@執迷</a>
        <span> in </span>
        <span class="post_category"><a href="./" rel="bookmark" title="Permalink to 自然語言處理">[ 自然語言處理 ]</a></span>
        <span class="post_date">2025-03-01</span>
        <div><span>Tags : </span>
            
            
            <span><a href="./">#Transformer, </a></span>
            
            <span><a href="./">#Linear Transformer, </a></span>
            
            <span><a href="./">#線性Transformer, </a></span>
            
            <span><a href="./">#RNN, </a></span>
            
            
        </div>

        <div class="entry-social-container" style="display: flex; justify-content: center; gap: 20px; margin-top: 20px;">
            
            
            <div class="entry-social" style="text-align: center;">
                <span class="social-text">分享本文</span><br>
                <div class="social-icons" style="display: flex; gap: 10px; justify-content: center;">
                    <span class="twitter"><a target="_blank" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=400,width=700');return false;" title="Twitter" href="https://twitter.com/share?url=././linear_transformer_20250215_zh-cn.html&text=【论文精读】Linear Transformer&via="><img src="./theme/images/icons/twitter-s.png"></a></span>
                    <a target="_blank" title="Linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=././linear_transformer_20250215_zh-cn.html&title=【论文精读】Linear Transformer" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=450,width=650');return false;"><img src="./theme/images/icons/linkedin-s.png"></a>
                    <span class="mail"><a href="mailto:?subject=【论文精读】Linear Transformer&amp;body=Viens découvrir un article à propos de [【论文精读】Linear Transformer] sur le site de 執迷. ././linear_transformer_20250215_zh-cn.html" title="Share by Email" target="_blank"><img src="./theme/images/icons/mail-s.png"></a></span>
                </div>
            </div>
        </div>
        </div>
    </div>
    

</section>

<!-- Quarto Clipboard -->
<script type="text/javascript">
    var clipboard = new ClipboardJS(
        '.code-copy-button',
        {
            target: function(trigger) {
                console.log("Copy texts. ", trigger);
                return trigger.previousElementSibling;
            }
        }
    );    
    clipboard.on('success', function(e) {
        console.info('Action:', e.action);
        console.info('Text:', e.text);
        console.info('Trigger:', e.trigger);
        e.clearSelection();
        alert('Copied!');
    });
    console.log("ClipboardJS loaded.");
</script>

  </article>

  <!-- Footer -->
  <footer>
    <p>
      Blog powered by <a href="http://getpelican.com/">Pelican</a>, 
      which takes great advantage of <a href="http://python.org">Python</a>.
      Theme <a href="https://github.com/parbhat/pelican-blue">Pelican-Blue</a> by <a href="https://parbhatpuri.com/">@parbhat</a>.
    </p>
    
      
      <span id="busuanzi_container_site_pv">總訪問量<span id="busuanzi_value_site_pv"></span></span>
      <span id="busuanzi_container_site_uv">
        訪客數<span id="busuanzi_value_site_uv"></span>
      </span>
      
    
  </footer>
</body>
</html>