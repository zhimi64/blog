<!doctype html>
<html lang="" itemscope itemtype="http://schema.org/Person">
<head>

  
  <!-- Google tag (gtag.js) -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-G3N739QVFZ"></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
  
    gtag('config', 'G-G3N739QVFZ');
  </script>
  

  
  <script>
    window.va = window.va || function () { (window.vaq = window.vaq || []).push(arguments); };
  </script>
  <script defer src="./_vercel/insights/script.js"></script>
  

  
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
  
  
  

<!-- Quarto -->
<link href="./theme/css/quarto/quarto.css" rel="stylesheet">
<script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.11/dist/clipboard.min.js"></script>
<script src="./theme/scripts/quarto/quarto.js"></script>


  <meta charset="utf-8">
  <!-- Site Meta Data -->
  <title>批歸一化（Batch Normalization）原理分析和代碼實現</title>
  
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="description" content="">
  <meta name="author" content="執迷">

  <link rel="shortcut icon" href="">

  <!-- schema.org -->
  <meta itemprop="name" content="執迷的博客">
  <meta itemprop="image" content="">
  <meta itemprop="description" content="">

  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,600,700' rel='stylesheet' type='text/css'>
  <!-- Style Meta Data -->
  <link rel="stylesheet" href="./theme/css/style.css" type="text/css" />
  <link rel="stylesheet" href="./theme/css/pygments.css" type="text/css" />

  <!-- Feed Meta Data -->
  
  

  <!-- Twitter Feed -->
  <meta name="twitter:card" content="summary">
  <meta name="twitter:site" content="">
  <meta name="twitter:image" content="">
  
<meta name="twitter:creator" content="">
<meta name="twitter:url" content="././bn_20240318.html">
<meta name="twitter:title" content="執迷的博客 ~ 批歸一化（Batch Normalization）原理分析和代碼實現">
<meta name="twitter:description" content="">

<!-- Facebook Meta Data -->
<meta property="og:title" content="執迷的博客 ~ 批歸一化（Batch Normalization）原理分析和代碼實現" />
<meta property="og:description" content="" />
<meta property="og:image" content="" />


<!-- MathJax -->

<script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>




</head>

<body>
  <!-- Sidebar -->
  <aside>
    <!--<center><a href="."><img id="avatar" src=""></a></center>-->
    <h1>執迷的博客</h1>
    
      <p>做一個有趣的人！</p>
    
    <br>

    

    <nav class="nav">
      <ul class="list-bare">
      
        
          <li><a class="nav__link" href="./index.html">Blog</a></li>
        
         
        
          <li><a class="nav__link" href="././about.html">About</a></li>
        
         
      </ul>
    </nav>

    <p class="social">
      
      
      
    </p>

    <!--
    
    -->
  </aside>

  <!-- Content -->
  <article>
    
<section id="content">
    <h2 class="post_title post_detail"><a href="././bn_20240318.html" rel="bookmark" title="Permalink to 批歸一化（Batch Normalization）原理分析和代碼實現">批歸一化（Batch Normalization）原理分析和代碼實現</a></h2>
    
    <p>
        <span id="busuanzi_container_page_pv">
            本文瀏覽次數<span id="busuanzi_value_page_pv"></span>
        </span>
    </p>
    
    <div class="entry-content blog-post">
        <div>
<div class="page-columns page-rows-contents page-layout-article" id="quarto-content">
<div class="sidebar margin-sidebar" id="quarto-margin-sidebar">
<nav class="toc-active" id="TOC" role="doc-toc">
<h2 id="toc-title">目录</h2>
<ul>
<li><a class="nav-link active" data-scroll-target="#簡介" href="#簡介" id="toc-簡介"><span class="toc-section-number">1</span>  簡介</a></li>
<li><a class="nav-link" data-scroll-target="#實現方法" href="#實現方法" id="toc-實現方法"><span class="toc-section-number">2</span>  實現方法</a></li>
<li><a class="nav-link" data-scroll-target="#代碼實現" href="#代碼實現" id="toc-代碼實現"><span class="toc-section-number">3</span>  代碼實現</a></li>
<li><a class="nav-link" data-scroll-target="#正確地使用bn" href="#正確地使用bn" id="toc-正確地使用bn"><span class="toc-section-number">4</span>  正確地使用BN</a>
<ul class="collapse">
<li><a class="nav-link" data-scroll-target="#bn在訓練階段與測試階段的行為差異" href="#bn在訓練階段與測試階段的行為差異" id="toc-bn在訓練階段與測試階段的行為差異"><span class="toc-section-number">4.1</span>  BN在訓練階段與測試階段的行為差異</a></li>
<li><a class="nav-link" data-scroll-target="#如何正確地凍結bn模塊" href="#如何正確地凍結bn模塊" id="toc-如何正確地凍結bn模塊"><span class="toc-section-number">4.2</span>  如何正確地凍結BN模塊</a></li>
<li><a class="nav-link" data-scroll-target="#分佈式訓練" href="#分佈式訓練" id="toc-分佈式訓練"><span class="toc-section-number">4.3</span>  分佈式訓練</a></li>
<li><a class="nav-link" data-scroll-target="#不要遞歸地使用bn" href="#不要遞歸地使用bn" id="toc-不要遞歸地使用bn"><span class="toc-section-number">4.4</span>  不要遞歸地使用BN</a></li>
</ul></li>
<li><a class="nav-link" data-scroll-target="#總結" href="#總結" id="toc-總結"><span class="toc-section-number">5</span>  總結</a></li>
</ul>
</nav>
</div>
<main class="content" id="quarto-document-content">
<header class="quarto-title-block default" id="title-block-header">
<div class="quarto-title">

</div>
<div class="quarto-title-meta">
<div>
<div class="quarto-title-meta-heading">发布日期</div>
<div class="quarto-title-meta-contents">
<p class="date">2024年3月18日</p>
</div>
</div>
</div>
</header>
<section class="level2" data-number="1" id="簡介">
<h2 class="anchored" data-anchor-id="簡介" data-number="1"><span class="header-section-number">1</span> 簡介</h2>
<p>批歸一化（Batch Normalization，BN）是在2015年由Sergey Loffe和Christan Szegedy<span class="citation" data-cites="ioffe_batch_nodate"><sup>[<a href="#ref-ioffe_batch_nodate" role="doc-biblioref">1</a>]</sup></span>提出的一種加速深度學習模型收斂的方法。</p>
<p>在模型的訓練過程中，每個深度學習模型每一層模塊的輸出分佈都在不斷變化，後續的模塊需要不斷適應新的輸入模式，這個問題在Batch Normalization<span class="citation" data-cites="ioffe_batch_nodate"><sup>[<a href="#ref-ioffe_batch_nodate" role="doc-biblioref">1</a>]</sup></span>中被稱為<em>internal covariate shift</em>。為克服這個問題，Batch Normalization提出在模型內部加入歸一化層。歸一化層的引入使得模型的訓練更加穩定，允許使用更大的學習率，使得模型對參數的初始化沒那麼敏感。</p>
<p>本文主要談一談BN的運行原理和實現細節，最後分享一些BN的使用經驗和容易踩到的坑。</p>
</section>
<section class="level2" data-number="2" id="實現方法">
<h2 class="anchored" data-anchor-id="實現方法" data-number="2"><span class="header-section-number">2</span> 實現方法</h2>
<p>BN使用如下公式實現輸入樣本的歸一化： <span id="eq-bn-normalization"><span class="math display">\[
\hat x = \frac{x - E(x)}{\sqrt{Var(x) + \epsilon}},
\tag{1}\]</span></span> 其中<span class="math inline">\(x\)</span>為BN模塊的輸入。<span class="math inline">\(E(x)\)</span>為<span class="math inline">\(x\)</span>的數學期望，<span class="math inline">\(Var(x)\)</span>為<span class="math inline">\(x\)</span>的方差，<span class="math inline">\(\epsilon\)</span>是防止除零異常的一個接近<span class="math inline">\(0\)</span>的正數。</p>
<p>隨後，歸一化的樣本經過一層線性層得到BN的輸出： <span class="math display">\[
y = \hat x\gamma  + \beta.
\]</span></p>
<p>實際訓練時的tensor是N維的。例如一組圖像的特徵<span class="math inline">\(x\)</span>一般是4維，形狀可能是<span class="math inline">\(8\times 128 \times 512 \times 512\)</span>，其中<span class="math inline">\(8\)</span>為batch size，<span class="math inline">\(128\)</span>為特徵維度，<span class="math inline">\(512\)</span>為圖像的長寬。在這個例子中，BN的歸一化針對所有長度為<span class="math inline">\(128\)</span>的特徵向量進行，其統計的<span class="math inline">\(E(x)\)</span>和<span class="math inline">\(Var(x)\)</span>是維度為<span class="math inline">\(128\)</span>的向量。</p>
<p>在代碼實現中，<span class="math inline">\(E(x)\)</span>和<span class="math inline">\(Var(x)\)</span>通過統計一個batch內樣本的均值和方差來近似。為了獲得盡量準確的統計，batch size最好取盡量大些。如果batch size太小，那麼<span class="math inline">\(E(x)\)</span>和<span class="math inline">\(Var(x)\)</span>的估計不準確，模型的最終性能便可能下降。</p>
<p>測試推理階段，模型往往一次接受一個數據：<span class="math inline">\(\text{batch size}=1\)</span>。因此在推理階段，BN就不能像訓練時那樣用大量數據估計<span class="math inline">\(E(x)\)</span>和<span class="math inline">\(Var(x)\)</span>. 為了應對這個問題，BN的設計者提出讓<strong>Batch Normalization在測試階段和訓練階段採取不一樣的行為</strong>。在訓練階段，BN使用Batch內統計的均值和方差，同時使用moving average方法維護均值和方差的統計量。設BN的<code>momentum</code>參數等於0.1，<span class="math inline">\(m\)</span>是moving average方法跟蹤的一個統計量，那麼其更新方法為： <span class="math display">\[
\hat m_{t} = \hat m_{t-1} \cdot (1 - \text{momentum}) + m_t \cdot \text{momentum}.
\]</span></p>
<p>既然測試時我們無法構造一個大的batch來統計均值和方差，那就用事先統計的均值和方差的moving average，帶入<a href="#eq-bn-normalization">公式 1</a>中進行歸一化。</p>
</section>
<section class="level2" data-number="3" id="代碼實現">
<h2 class="anchored" data-anchor-id="代碼實現" data-number="3"><span class="header-section-number">3</span> 代碼實現</h2>
<p>在復刻BatchNorm之前，我們不妨先看看pytorch官方的<code>BatchNorm2d</code>模塊，觀察BatchNorm層要有哪些參數：</p>
<div class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a aria-hidden="true" href="#cb1-1" tabindex="-1"></a><span class="im">import</span> torch </span>
<span id="cb1-2"><a aria-hidden="true" href="#cb1-2" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn </span>
<span id="cb1-3"><a aria-hidden="true" href="#cb1-3" tabindex="-1"></a>batch, ch, h, w <span class="op">=</span> <span class="dv">2</span>, <span class="dv">32</span>, <span class="dv">128</span>, <span class="dv">128</span></span>
<span id="cb1-4"><a aria-hidden="true" href="#cb1-4" tabindex="-1"></a>torch_batch_norm <span class="op">=</span> nn.BatchNorm2d(num_features<span class="op">=</span>ch)</span>
<span id="cb1-5"><a aria-hidden="true" href="#cb1-5" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> torch_batch_norm.named_parameters():</span>
<span id="cb1-6"><a aria-hidden="true" href="#cb1-6" tabindex="-1"></a>    <span class="bu">print</span>(k, v.shape)</span>
<span id="cb1-7"><a aria-hidden="true" href="#cb1-7" tabindex="-1"></a><span class="cf">for</span> k, v <span class="kw">in</span> torch_batch_norm.named_buffers():</span>
<span id="cb1-8"><a aria-hidden="true" href="#cb1-8" tabindex="-1"></a>    <span class="bu">print</span>(k, v.shape)</span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>weight torch.Size([32])
bias torch.Size([32])
running_mean torch.Size([32])
running_var torch.Size([32])
num_batches_tracked torch.Size([])</code></pre>
</div>
</div>
<p>我們注意到BatchNorm的參數有兩種。一種是<em>parameter</em>（<code>weight</code>、<code>bias</code>），一種是<em>buffer</em>（<code>running_mean</code>、<code>running_var</code>、<code>num_batches_tracked</code>）。對於parameter，torch默認其參數是需要梯度反傳的；而buffer則用於存儲一些不需要梯度反傳的模型參數。與parameter一樣，在保存模型時，<code>buffer</code>參數也會存儲到<code>state_dict</code>中。</p>
<p>下面是本文提供的BN實現：</p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a aria-hidden="true" href="#cb3-1" tabindex="-1"></a><span class="im">import</span> torch</span>
<span id="cb3-2"><a aria-hidden="true" href="#cb3-2" tabindex="-1"></a><span class="im">import</span> torch.nn <span class="im">as</span> nn </span>
<span id="cb3-3"><a aria-hidden="true" href="#cb3-3" tabindex="-1"></a><span class="kw">class</span> MyBatchNorm(nn.Module):</span>
<span id="cb3-4"><a aria-hidden="true" href="#cb3-4" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, num_features, eps<span class="op">=</span><span class="fl">1e-5</span>, momentum<span class="op">=</span><span class="fl">0.1</span>):</span>
<span id="cb3-5"><a aria-hidden="true" href="#cb3-5" tabindex="-1"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-6"><a aria-hidden="true" href="#cb3-6" tabindex="-1"></a>        <span class="co"># weight初始化為1，bias初始化為0.</span></span>
<span id="cb3-7"><a aria-hidden="true" href="#cb3-7" tabindex="-1"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(torch.ones(num_features))</span>
<span id="cb3-8"><a aria-hidden="true" href="#cb3-8" tabindex="-1"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(num_features))</span>
<span id="cb3-9"><a aria-hidden="true" href="#cb3-9" tabindex="-1"></a></span>
<span id="cb3-10"><a aria-hidden="true" href="#cb3-10" tabindex="-1"></a>        <span class="va">self</span>.momentum <span class="op">=</span> <span class="fl">0.1</span> </span>
<span id="cb3-11"><a aria-hidden="true" href="#cb3-11" tabindex="-1"></a>        <span class="va">self</span>.eps <span class="op">=</span> eps </span>
<span id="cb3-12"><a aria-hidden="true" href="#cb3-12" tabindex="-1"></a></span>
<span id="cb3-13"><a aria-hidden="true" href="#cb3-13" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'running_mean'</span>, torch.zeros(num_features))</span>
<span id="cb3-14"><a aria-hidden="true" href="#cb3-14" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'running_var'</span>, torch.ones(num_features))</span>
<span id="cb3-15"><a aria-hidden="true" href="#cb3-15" tabindex="-1"></a>        <span class="va">self</span>.register_buffer(<span class="st">'num_batches_tracked'</span>, torch.tensor(<span class="fl">0.</span>))</span>
<span id="cb3-16"><a aria-hidden="true" href="#cb3-16" tabindex="-1"></a></span>
<span id="cb3-17"><a aria-hidden="true" href="#cb3-17" tabindex="-1"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, tensor): </span>
<span id="cb3-18"><a aria-hidden="true" href="#cb3-18" tabindex="-1"></a>        bs, ch, h, w <span class="op">=</span> tensor.shape </span>
<span id="cb3-19"><a aria-hidden="true" href="#cb3-19" tabindex="-1"></a>        <span class="co"># 變換一下tensor的尺寸，方便處理</span></span>
<span id="cb3-20"><a aria-hidden="true" href="#cb3-20" tabindex="-1"></a>        tensor_flatten <span class="op">=</span> tensor.permute(<span class="dv">0</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">1</span>).flatten(<span class="dv">0</span>, <span class="dv">2</span>)  <span class="co"># bs * h * w, ch </span></span>
<span id="cb3-21"><a aria-hidden="true" href="#cb3-21" tabindex="-1"></a>        <span class="co"># 求均值和方差</span></span>
<span id="cb3-22"><a aria-hidden="true" href="#cb3-22" tabindex="-1"></a>        mean <span class="op">=</span> torch.mean(tensor_flatten, <span class="dv">0</span>)</span>
<span id="cb3-23"><a aria-hidden="true" href="#cb3-23" tabindex="-1"></a>        <span class="co"># 注意方差有biased和unbiased兩種。</span></span>
<span id="cb3-24"><a aria-hidden="true" href="#cb3-24" tabindex="-1"></a>        var <span class="op">=</span> torch.var(tensor_flatten, <span class="dv">0</span>, unbiased<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-25"><a aria-hidden="true" href="#cb3-25" tabindex="-1"></a>        var_unbiased <span class="op">=</span> torch.var(tensor_flatten, <span class="dv">0</span>, unbiased<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb3-26"><a aria-hidden="true" href="#cb3-26" tabindex="-1"></a></span>
<span id="cb3-27"><a aria-hidden="true" href="#cb3-27" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb3-28"><a aria-hidden="true" href="#cb3-28" tabindex="-1"></a>            <span class="co"># 訓練時，我們要執行moving average，統計</span></span>
<span id="cb3-29"><a aria-hidden="true" href="#cb3-29" tabindex="-1"></a>            <span class="co"># running_mean和running_var，注意此時應</span></span>
<span id="cb3-30"><a aria-hidden="true" href="#cb3-30" tabindex="-1"></a>            <span class="co"># 使用unbiased版本的方差。</span></span>
<span id="cb3-31"><a aria-hidden="true" href="#cb3-31" tabindex="-1"></a>            <span class="va">self</span>.running_mean.mul_(<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum).add_(<span class="va">self</span>.momentum <span class="op">*</span> mean)</span>
<span id="cb3-32"><a aria-hidden="true" href="#cb3-32" tabindex="-1"></a>            <span class="va">self</span>.running_var.mul_(<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.momentum).add_(<span class="va">self</span>.momentum <span class="op">*</span> var_unbiased)</span>
<span id="cb3-33"><a aria-hidden="true" href="#cb3-33" tabindex="-1"></a>            <span class="va">self</span>.num_batches_tracked.add_(<span class="dv">1</span>)</span>
<span id="cb3-34"><a aria-hidden="true" href="#cb3-34" tabindex="-1"></a></span>
<span id="cb3-35"><a aria-hidden="true" href="#cb3-35" tabindex="-1"></a>        <span class="co"># 訓練時用batch內的統計量，測試時用moving average</span></span>
<span id="cb3-36"><a aria-hidden="true" href="#cb3-36" tabindex="-1"></a>        <span class="co"># 保存的統計量。</span></span>
<span id="cb3-37"><a aria-hidden="true" href="#cb3-37" tabindex="-1"></a>        <span class="cf">if</span> <span class="va">self</span>.training:</span>
<span id="cb3-38"><a aria-hidden="true" href="#cb3-38" tabindex="-1"></a>            tensor_flatten <span class="op">=</span> (tensor_flatten <span class="op">-</span> mean) <span class="op">/</span> torch.sqrt(var <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb3-39"><a aria-hidden="true" href="#cb3-39" tabindex="-1"></a>        <span class="cf">else</span>: </span>
<span id="cb3-40"><a aria-hidden="true" href="#cb3-40" tabindex="-1"></a>            tensor_flatten <span class="op">=</span> (tensor_flatten <span class="op">-</span> <span class="va">self</span>.running_mean) <span class="op">/</span> torch.sqrt(<span class="va">self</span>.running_var <span class="op">+</span> <span class="va">self</span>.eps)</span>
<span id="cb3-41"><a aria-hidden="true" href="#cb3-41" tabindex="-1"></a></span>
<span id="cb3-42"><a aria-hidden="true" href="#cb3-42" tabindex="-1"></a>        <span class="co"># 歸一化完成後，做線性變換。</span></span>
<span id="cb3-43"><a aria-hidden="true" href="#cb3-43" tabindex="-1"></a>        ret <span class="op">=</span> tensor_flatten <span class="op">*</span> <span class="va">self</span>.weight <span class="op">+</span> <span class="va">self</span>.bias </span>
<span id="cb3-44"><a aria-hidden="true" href="#cb3-44" tabindex="-1"></a>        ret <span class="op">=</span> ret.view(bs, h, w, ch).permute(<span class="dv">0</span>, <span class="dv">3</span>, <span class="dv">1</span>, <span class="dv">2</span>)</span>
<span id="cb3-45"><a aria-hidden="true" href="#cb3-45" tabindex="-1"></a>        <span class="cf">return</span> ret </span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
</div>
<p>接下來驗證看看<code>MyBatchNorm</code>的行為和<code>torch.nn.BatchNorm2d</code>是否完全一致。</p>
<p>我們先檢查訓練模式下兩者的行為：</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a aria-hidden="true" href="#cb4-1" tabindex="-1"></a>my_batch_norm <span class="op">=</span> MyBatchNorm(num_features<span class="op">=</span>ch)</span>
<span id="cb4-2"><a aria-hidden="true" href="#cb4-2" tabindex="-1"></a><span class="co"># 因為BN涉及running_mean和running_var的更新，所以我們要多跑幾輪來檢查moving average的正確性。</span></span>
<span id="cb4-3"><a aria-hidden="true" href="#cb4-3" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb4-4"><a aria-hidden="true" href="#cb4-4" tabindex="-1"></a>    a <span class="op">=</span> torch.rand(batch, ch, h, w)</span>
<span id="cb4-5"><a aria-hidden="true" href="#cb4-5" tabindex="-1"></a>    ret1 <span class="op">=</span> torch_batch_norm(a)</span>
<span id="cb4-6"><a aria-hidden="true" href="#cb4-6" tabindex="-1"></a>    ret2 <span class="op">=</span> my_batch_norm(a)</span>
<span id="cb4-7"><a aria-hidden="true" href="#cb4-7" tabindex="-1"></a>    diff <span class="op">=</span> torch.mean(torch.<span class="bu">abs</span>(ret1 <span class="op">-</span> ret2)).item()</span>
<span id="cb4-8"><a aria-hidden="true" href="#cb4-8" tabindex="-1"></a>    </span>
<span id="cb4-9"><a aria-hidden="true" href="#cb4-9" tabindex="-1"></a>    running_mean1 <span class="op">=</span> torch_batch_norm.running_mean </span>
<span id="cb4-10"><a aria-hidden="true" href="#cb4-10" tabindex="-1"></a>    running_mean2 <span class="op">=</span> my_batch_norm.running_mean </span>
<span id="cb4-11"><a aria-hidden="true" href="#cb4-11" tabindex="-1"></a>    diff_mean <span class="op">=</span> torch.mean(torch.<span class="bu">abs</span>(running_mean1 <span class="op">-</span> running_mean2)).item()</span>
<span id="cb4-12"><a aria-hidden="true" href="#cb4-12" tabindex="-1"></a></span>
<span id="cb4-13"><a aria-hidden="true" href="#cb4-13" tabindex="-1"></a>    running_var1 <span class="op">=</span> torch_batch_norm.running_var </span>
<span id="cb4-14"><a aria-hidden="true" href="#cb4-14" tabindex="-1"></a>    running_var2 <span class="op">=</span> my_batch_norm.running_var</span>
<span id="cb4-15"><a aria-hidden="true" href="#cb4-15" tabindex="-1"></a>    diff_var <span class="op">=</span> torch.mean(torch.<span class="bu">abs</span>(running_var1 <span class="op">-</span> running_var2)).item()</span>
<span id="cb4-16"><a aria-hidden="true" href="#cb4-16" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{:.6f}</span><span class="st">;</span><span class="sc">{:.6f}</span><span class="st">;</span><span class="sc">{:.6f}</span><span class="st">'</span>.<span class="bu">format</span>(diff, diff_mean, diff_var))</span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.000000;0.000000;0.000000
0.000000;0.000000;0.000000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.000000;0.000000;0.000000
0.000000;0.000000;0.000000
0.000000;0.000000;0.000000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.000000;0.000000;0.000000
0.000000;0.000000;0.000000
0.000000;0.000000;0.000000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.000000;0.000000;0.000000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.000000;0.000000;0.000000</code></pre>
</div>
</div>
<p>可以看到，輸出的所有誤差項都為0！這表明<code>MyBatchNorm</code>的實現和torch的BN相吻合。</p>
<p>不要忘記BN在訓練時的行為和測試時的行為不同。我們需要再檢查一遍測試階段下<code>MyBatchNorm</code>的行為。</p>
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a aria-hidden="true" href="#cb11-1" tabindex="-1"></a><span class="co"># .eval()開啟測試模型</span></span>
<span id="cb11-2"><a aria-hidden="true" href="#cb11-2" tabindex="-1"></a>torch_batch_norm.<span class="bu">eval</span>()  </span>
<span id="cb11-3"><a aria-hidden="true" href="#cb11-3" tabindex="-1"></a>my_batch_norm.<span class="bu">eval</span>()</span>
<span id="cb11-4"><a aria-hidden="true" href="#cb11-4" tabindex="-1"></a><span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">10</span>):</span>
<span id="cb11-5"><a aria-hidden="true" href="#cb11-5" tabindex="-1"></a>    a <span class="op">=</span> torch.rand(batch, ch, h, w)</span>
<span id="cb11-6"><a aria-hidden="true" href="#cb11-6" tabindex="-1"></a>    ret1 <span class="op">=</span> torch_batch_norm(a)</span>
<span id="cb11-7"><a aria-hidden="true" href="#cb11-7" tabindex="-1"></a>    ret2 <span class="op">=</span> my_batch_norm(a)</span>
<span id="cb11-8"><a aria-hidden="true" href="#cb11-8" tabindex="-1"></a>    diff <span class="op">=</span> torch.mean(torch.<span class="bu">abs</span>(ret1 <span class="op">-</span> ret2)).item()</span>
<span id="cb11-9"><a aria-hidden="true" href="#cb11-9" tabindex="-1"></a>    </span>
<span id="cb11-10"><a aria-hidden="true" href="#cb11-10" tabindex="-1"></a>    running_mean1 <span class="op">=</span> torch_batch_norm.running_mean </span>
<span id="cb11-11"><a aria-hidden="true" href="#cb11-11" tabindex="-1"></a>    running_mean2 <span class="op">=</span> my_batch_norm.running_mean </span>
<span id="cb11-12"><a aria-hidden="true" href="#cb11-12" tabindex="-1"></a>    diff_mean <span class="op">=</span> torch.mean(torch.<span class="bu">abs</span>(running_mean1 <span class="op">-</span> running_mean2)).item()</span>
<span id="cb11-13"><a aria-hidden="true" href="#cb11-13" tabindex="-1"></a></span>
<span id="cb11-14"><a aria-hidden="true" href="#cb11-14" tabindex="-1"></a>    running_var1 <span class="op">=</span> torch_batch_norm.running_var </span>
<span id="cb11-15"><a aria-hidden="true" href="#cb11-15" tabindex="-1"></a>    running_var2 <span class="op">=</span> my_batch_norm.running_var</span>
<span id="cb11-16"><a aria-hidden="true" href="#cb11-16" tabindex="-1"></a>    diff_var <span class="op">=</span> torch.mean(torch.<span class="bu">abs</span>(running_var1 <span class="op">-</span> running_var2)).item()</span>
<span id="cb11-17"><a aria-hidden="true" href="#cb11-17" tabindex="-1"></a>    <span class="bu">print</span>(<span class="st">'</span><span class="sc">{:.6f}</span><span class="st">;</span><span class="sc">{:.6f}</span><span class="st">;</span><span class="sc">{:.6f}</span><span class="st">'</span>.<span class="bu">format</span>(diff, diff_mean, diff_var))</span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>0.000000;0.000000;0.000000
0.000000;0.000000;0.000000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
0.000000;0.000000;0.000000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
0.000000;0.000000;0.000000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.000000;0.000000;0.000000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.000000;0.000000;0.000000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.000000;0.000000;0.000000
0.000000;0.000000;0.000000
0.000000;0.000000;0.000000</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code></code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>0.000000;0.000000;0.000000</code></pre>
</div>
</div>
<p>至此我們已經完成了全部檢查，實驗結果表明<code>MyBatchNorm</code>的實現是正確的。</p>
</section>
<section class="level2" data-number="4" id="正確地使用bn">
<h2 class="anchored" data-anchor-id="正確地使用bn" data-number="4"><span class="header-section-number">4</span> 正確地使用BN</h2>
<p>BN作為一種實用、應用廣泛的歸一化模塊，是計算機視覺領域的一座里程碑。儘管BN的應用確實解決了一些實際問題，但它也存在一些“坑”，是在使用BN時應當注意的。</p>
<section class="level3" data-number="4.1" id="bn在訓練階段與測試階段的行為差異">
<h3 class="anchored" data-anchor-id="bn在訓練階段與測試階段的行為差異" data-number="4.1"><span class="header-section-number">4.1</span> BN在訓練階段與測試階段的行為差異</h3>
<p>在訓練階段，BN使用batch內統計的均值和方差作歸一化，並記錄它們的moving average；而在測試階段，BN不再統計新數據的均值和方差，也不再更新moving average。這種不一致性（inconsistency）在後續工作<span class="citation" data-cites="wu_group_2018"><sup>[<a href="#ref-wu_group_2018" role="doc-biblioref">2</a>]</sup></span>中被認為是一種影響性能的潛在因素。</p>
</section>
<section class="level3" data-number="4.2" id="如何正確地凍結bn模塊">
<h3 class="anchored" data-anchor-id="如何正確地凍結bn模塊" data-number="4.2"><span class="header-section-number">4.2</span> 如何正確地凍結BN模塊</h3>
<p>設想我們有一個模型經過了充分的預訓練，現在我們希望在一個小數據集上微調它。一般步驟包括（以pytorch為例）：</p>
<ol type="1">
<li>阻止梯度反傳。這可以通過使用<code>torch.no_grad()</code>或將該各參數的<code>requires_grad</code>屬性設置為<code>False</code>做到；</li>
<li>調用<code>module.eval()</code>，關閉<code>train</code>模式；</li>
</ol>
<p>針對第2點，一般人們有兩種意見。一種看法認為不開BN的<code>eval</code>模式更好，這有助於讓模型學習如何對新數據做歸一化。而我傾向於採取的做法則是開啟<code>eval</code>。在我的經驗中，如果BN處於訓練狀態，而模型的其它層則凍結著，那麼模型可能因為不適應BN在新數據上歸一化參數的改變而引發訓練不穩定。</p>
<p>總而言之，BN在訓練、遷移學習、測試時的行為不一致有時確實是一個麻煩的問題。如果遇到了這個問題，我建議考慮一下是否要開啟BN的<code>eval</code>模式，或者試試後來的Group Normalization<span class="citation" data-cites="wu_group_2018"><sup>[<a href="#ref-wu_group_2018" role="doc-biblioref">2</a>]</sup></span>。</p>
</section>
<section class="level3" data-number="4.3" id="分佈式訓練">
<h3 class="anchored" data-anchor-id="分佈式訓練" data-number="4.3"><span class="header-section-number">4.3</span> 分佈式訓練</h3>
<p>在訓練參數量較大的模型時，可以用分佈式訓練，利用多個進程和多個計算設備執行計算。這種情況下，每張卡只需負責比較小的batch。注意原始的BN在batch size較小時，所產生的均值/方差的統計量不準確。因此，在分佈式訓練時，我們最好將原BatchNorm模塊替換為<code>torch.SyncBatchNorm</code>。後者能同步所有計算設備，在更大的batch size上統計均值和方差。</p>
</section>
<section class="level3" data-number="4.4" id="不要遞歸地使用bn">
<h3 class="anchored" data-anchor-id="不要遞歸地使用bn" data-number="4.4"><span class="header-section-number">4.4</span> 不要遞歸地使用BN</h3>
<p>最後介紹一個我踩過的，印象深刻的坑。 假如有這樣一段代碼：</p>
<div class="sourceCode" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a aria-hidden="true" href="#cb20-1" tabindex="-1"></a>x2 <span class="op">=</span> batch_norm(x1)</span>
<span id="cb20-2"><a aria-hidden="true" href="#cb20-2" tabindex="-1"></a>x3 <span class="op">=</span> batch_norm(x2)</span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
<p>或</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a aria-hidden="true" href="#cb21-1" tabindex="-1"></a>x2 <span class="op">=</span> conv(x1)  <span class="co"># conv中包含BN模塊</span></span>
<span id="cb21-2"><a aria-hidden="true" href="#cb21-2" tabindex="-1"></a>x3 <span class="op">=</span> conv(x2)</span></code><button class="code-copy-button" title="复制到剪贴板"><i class="bi"></i></button></pre></div>
<p>前面的一段代碼的問題或許容易識別，後者的問題則稍隱蔽些。你能預測到會發生什麼嗎？在這樣的代碼中，同一個BN模塊在訓練時會分別獲取<code>x2</code>和<code>x1</code>的均值和方差，然後通過moving average將它們計入<code>running_mean</code>和<code>running_var</code>。然而，由於<code>x1</code>和<code>x2</code>服從不同的分佈，因此<code>running_mean</code>和<code>running_var</code>的統計將失去意義。 問題的表現是，在訓練階段，我們會觀察到損失正常下降。測試時，我們開啟<code>eval</code>模式，模型的表現不如預期；可是如果你關閉<code>eval</code>模式，也許會發現模型又能正常工作。</p>
<p>類似的問題也存在於特征金字塔（FPN）的實現中。如果你希望在類特征金字塔的結構中實現不同層級共享參數的話，注意卷積的參數也許能共享，但BN的參數不要共享。</p>
</section>
</section>
<section class="level2" data-number="5" id="總結">
<h2 class="anchored" data-anchor-id="總結" data-number="5"><span class="header-section-number">5</span> 總結</h2>
<p>本文介紹了BN的工作原理，給出了一種基於pytorch的BN模塊實現，並提供了詳細的代碼檢查。最後，本文討論了應用BN過程中容易遇到的幾種問題。</p>
<p>在接觸深度學習的過程中，Batch Normalization是一個反復（大概得有兩三次吧）讓我踩坑的模塊，每次踩坑都得琢磨好久才能發現問題所在。現在我已經習慣性的選擇Group Normalization<span class="citation" data-cites="wu_group_2018"><sup>[<a href="#ref-wu_group_2018" role="doc-biblioref">2</a>]</sup></span>，拋棄BN了。儘管如此，BN仍然是一個經典的工作，在折騰它的過程中我也學到了不少東西。</p>
</section>
<div class="default" id="quarto-appendix"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">参考</h2><div class="references csl-bib-body" id="refs" role="doc-bibliography">
<div class="csl-entry" id="ref-ioffe_batch_nodate" role="doc-biblioentry">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">IOFFE S, SZEGEDY C. Batch <span>Normalization</span>: <span>Accelerating</span> <span>Deep</span> <span>Network</span> <span>Training</span> by <span>Reducing</span> <span>Internal</span> <span>Covariate</span> <span>Shift</span>[J]. : 9.</div>
</div>
<div class="csl-entry" id="ref-wu_group_2018" role="doc-biblioentry">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">WU Y, HE K. <a href="http://arxiv.org/abs/1803.08494">Group <span>Normalization</span></a>[J]. arXiv:1803.08494 [cs], 2018.</div>
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "已复制");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "已复制");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
</div>
    </div>
    <div class="post_list">
        <span>By </span>
        <a href="./">@執迷</a>
        <span> in </span>
        <span class="post_category"><a href="./" rel="bookmark" title="Permalink to 深度學習">[ 深度學習 ]</a></span>
        <span class="post_date">2024-03-18</span>
        <div><span>Tags : </span>
            
            
            <span><a href="./">#Batch Normalization, </a></span>
            
            <span><a href="./">#Layer Normalization, </a></span>
            
            <span><a href="./">#歸一化方法, </a></span>
            
            <span><a href="./">#自然語言處理, </a></span>
            
            <span><a href="./">#計算機視覺, </a></span>
            
            
        </div>

        <div class="entry-social">
            <span class="twitter"><a target="_blank" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=400,width=700');return false;" title="Twitter" href="https://twitter.com/share?url=././bn_20240318.html&text=批歸一化（Batch Normalization）原理分析和代碼實現&via="><img src="./theme/images/icons/twitter-s.png"></a></span>

            <span class="gplus"><a target="_blank" title="Google +" href="https://plus.google.com/share?url=././bn_20240318.html&hl=fr" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=450,width=650');return false;"><img src="./theme/images/icons/google-s.png"></a></span>

            <span class="facebook"><a target="_blank" title="Facebook" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=500,width=700');return false;" href="https://www.facebook.com/sharer.php?u=././bn_20240318.html&t=批歸一化（Batch Normalization）原理分析和代碼實現"><img src="./theme/images/icons/facebook-s.png"></a></span>

            <a  target="_blank" title="Linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=././bn_20240318.html&title=批歸一化（Batch Normalization）原理分析和代碼實現" rel="nofollow" onclick="javascript:window.open(this.href, '', 'menubar=no,toolbar=no,resizable=yes,scrollbars=yes,height=450,width=650');return false;"><img src="./theme/images/icons/linkedin-s.png"></a>

            <span class="mail"><a href="mailto:?subject=批歸一化（Batch Normalization）原理分析和代碼實現&amp;body=Viens découvrir un article à propos de [批歸一化（Batch Normalization）原理分析和代碼實現] sur le site de 執迷. ././bn_20240318.html" title="Share by Email" target="_blank"><img src="./theme/images/icons/mail-s.png"></a></span>
        </div>
    </div>
    

</section>

<!-- Quarto Clipboard -->
<script type="text/javascript">
    var clipboard = new ClipboardJS(
        '.code-copy-button',
        {
            target: function(trigger) {
                console.log("Copy texts. ", trigger);
                return trigger.previousElementSibling;
            }
        }
    );    
    clipboard.on('success', function(e) {
        console.info('Action:', e.action);
        console.info('Text:', e.text);
        console.info('Trigger:', e.trigger);
        e.clearSelection();
        alert('Copied!');
    });
    console.log("ClipboardJS loaded.");
</script>

  </article>

  <!-- Footer -->
  <footer>
    <p>
      Blog powered by <a href="http://getpelican.com/">Pelican</a>, 
      which takes great advantage of <a href="http://python.org">Python</a>.
      Theme <a href="https://github.com/parbhat/pelican-blue">Pelican-Blue</a> by <a href="https://parbhatpuri.com/">@parbhat</a>.
    </p>
    
      
      <span id="busuanzi_container_site_pv">總訪問量<span id="busuanzi_value_site_pv"></span></span>
      <span id="busuanzi_container_site_uv">
        訪客數<span id="busuanzi_value_site_uv"></span>
      </span>
      
    
  </footer>
</body>
</html>